{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "When using this notebook in **Playground mode**:\n",
        "- You can **run all cells** and see the results.\n",
        "- **Any changes you make will not be saved** after you close the notebook.\n",
        "\n",
        "If you’d like to make changes and save your work, please create a copy of this notebook in your own Google Drive:\n",
        "1. Go to **File > Save a copy in Drive**.\n",
        "2. This will create an editable version in your Drive, where you can modify and save your changes.\n",
        "\n",
        "If you ever want to reset the notebook to its original state, use this [link](https://colab.research.google.com/drive/1qdCe_t7aMTWtKmp5Ff3xWmmNpBcIlIVo#scrollTo=WXT8ZUvEKS8K&forceEdit=true&sandboxMode=true) to reopen it in Playground mode.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WXT8ZUvEKS8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge: Building Your First Computational Model That Processes Transitional Probabilities Between Syllables\n",
        "\n",
        "In this challenge, you will **build a simple computational model** that analyses transitional probabilities between syllables in a continuous stream of speech. This model will help simulate how infants might use a statistical learning mechanism to segment words from fluent speech, replicating the findings of Saffran et al. ([1996](https://doi.org/10.1126/science.274.5294.1926)).\n",
        "\n",
        "Saffran et al.'s study was groundbreaking and had a significant impact on the fields of statistical learning and cognitive science (e.g., Frost et al., [2019](https://doi.org/10.1037/bul0000210)). It provided compelling evidence that infants could use statistical cues to segment words from continuous speech. This finding challenged existing theories that emphasised innate linguistic knowledge, highlighting instead the role of experience-based learning mechanisms in language acquisition.\n",
        "\n",
        "Following this study, there was a surge of research investigating how statistical learning contributes to a wide range of cognitive functions, such as vision, audition, speech perception, reading, and motor planning, to name but a few. The study catalysed discussions on whether statistical learning is a domain-general mechanism and how it interacts with other cognitive processes. Overall, Saffran et al.'s work laid the foundation for a rich and expanding field that seeks to understand how humans (and even non-human species) extract regularities from their environment to facilitate learning and cognition.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TbokyOogYSXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Concepts\n",
        "\n",
        "- **Statistical Learning**: The ability to perceive and learn patterns in the environment that are either spatial or temporal in nature. (Frost et al., [2019](https://doi.org/10.1037/bul0000210)).\n",
        "\n",
        "- **Syllable Transitional Probability (TP)**: The probability of one syllable following another in a speech stream.\n",
        "\n",
        "- **Computational Model**: A set of mathematical and computational tools used to simulate a real-world system or process. In psychological science, computational models help formalise theories about mental processes by implementing them in code (Guest & Martin, [2021](https://doi.org/10.1177/1745691620970585)). Building a computational model involves:\n",
        "  1. **Theory Building**: Developing a conceptual understanding of the phenomenon.\n",
        "  2. **Specification**: Formalising the theory with mathematical equations or algorithms.\n",
        "  3. **Implementation**: Writing code to execute the model.\n",
        "  4. **Testing and Analysis**: Running the model and comparing predictions with real-world data.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sV_80Oj6b0hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "### The Experiment by Saffran et al. ([1996](https://doi.org/10.1126/science.274.5294.1926))\n",
        "\n",
        "Saffran and colleagues proposed that infants track **transitional probabilities (TPs)** between syllables to identify word boundaries in continuous speech.\n",
        "\n",
        "#### Experiment Overview\n",
        "\n",
        "- **Stimuli**: Four three-syllable nonsense words:\n",
        "  - \"tupiro\"\n",
        "  - \"golabu\"\n",
        "  - \"bidaku\"\n",
        "  - \"padoti\"\n",
        "\n",
        "- **Procedure**:\n",
        "  - **Familiarisation**: Infants were exposed to a continuous 2-minute stream of the nonsense words. Following a technique developed by Jusczyk and Aslin ([1995](https://doi.org/10.1006/cogp.1995.1010)), the stream contained 180 randomly selected words (540 syllables) without pauses between them, ensuring that the same word never occurred twice in a row.\n",
        "  \n",
        "  - **Testing**: 24 Infants' were tested on their ability to discriminate between words and “part-words”. The latter were sequences formed from the final syllable of one word and the first two syllables of another: \"rogola,\" \"bubida,\" \"kupado,\" and \"titupi\". The infants’ attention was directed to a speaker using a flashing red light. Words and part-words from the familiarisation sequence were repeatedly broadcast from the speaker, and the infants’ gaze duration at the speaker was recorded.\n",
        "\n",
        "   <img src=\"https://drive.google.com/uc?export=view&id=17cY_RBfF8AbM2eafigxyLs5gVAHNk0ie\" width=\"450px\" border=\"2px\">\n",
        "   \n",
        "   *AI-generated example image*\n",
        "\n",
        "\n",
        "#### Findings\n",
        "\n",
        "- Infants looked at the speaker significantly longer when they heard part-words compared with words, indicating they could distinguish between words and part-words based on TPs.\n",
        "- This behaviour suggests that infants possess a statistical learning mechanism that is sensitive to syllable TPs.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ffMjbW_ZdQra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Quick Look at Saffran et al.'s Results\n",
        "\n",
        "**Required Python Knowledge:**\n",
        "\n",
        "- **Basic Python Syntax:**\n",
        "  - Understanding code comments, variable assignments, lists, and function calls.\n",
        "\n",
        "- **Importing Libraries:**\n",
        "  - Familiarity with importing libraries and using aliases.\n",
        "\n",
        "- **Matplotlib Basics:**\n",
        "  - Creating bar charts with Matplotlib.\n",
        "  - Adjusting figure size, adding labels/titles, and configuring error bars."
      ],
      "metadata": {
        "id": "8Z32B0PcB6aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the experiment\n",
        "labels = ['Words', 'Part-words']\n",
        "mean_times = [7.97, 8.85]  # Mean fixation times (in seconds)\n",
        "se = [0.41 * 1.96, 0.45 * 1.96] # 95% confidence interval for each category\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(4, 3)) # Set figure size\n",
        "plt.bar(labels, mean_times, yerr=se, capsize=5) # Create bar chart with error bars\n",
        "plt.ylabel('Mean Fixation Time (s)') # Y-axis label\n",
        "plt.title(\"Saffran et al.'s (1996) results\")  # Title of the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XcI2P1mwczY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational Modelling Steps\n",
        "\n",
        "In this challenge, you will simulate the statistical learning mechanism proposed by Saffran et al.\n",
        "\n",
        "### Overview of Steps:\n",
        "\n",
        "1. **Data Generation**: Create 24 random continuous streams of the nonsense words (the same number as the infants tested in Saffran et al.'s study), following the stimuli construction rules from the original study.\n",
        "*Note*: In the original study, all infants heard the same continuous stream. Here, we generate a different random stream for each simulated infant, which is a nice way to rule out the possibility that performance is affected by the specific order of syllables in the stream.\n",
        "\n",
        "2. **Model Specification**: Define a computational model that can calculate transitional probabilities between syllables in a continuous stream.\n",
        "\n",
        "3. **Model Implementation**: Run 24 computational models, one for each of the continuous streams.\n",
        "\n",
        "4. **Analysis**:\n",
        "   - Demonstrate that TPs within words are higher than TPs between words within each continuous stream.\n",
        "   - Expose each model to the test items (words and part-words) and compute a score that could serve as a proxy for \"fixation time\".\n",
        "   - Compare the models' \"fixation\" times to those of infants from Saffran et al.'s study.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9dDkvG1HjS4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Data Generation\n",
        "\n",
        "**Objective**: Generate 24 random continuous streams of nonsense words, following the stimuli construction rules from Saffran et al.'s study.\n",
        "\n",
        "**Required Python Knowledge**:\n",
        "\n",
        "- Randomisation and Control of Random Seeds\n",
        "- String and List Manipulation\n",
        "- Conditional Statements and Loops\n",
        "- Function Definition and Parameter Use\n",
        "- List Comprehensions\n",
        "- Console Output for Debugging (e.g., printing intermediate results to verify that resulting objects meet expected specifications)"
      ],
      "metadata": {
        "id": "02QPDL2AyyHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list named \"words\" which contains the nonsense words\n",
        "# Pro Tips:\n",
        "# - The variable should be a list\n",
        "# - Each element should be a string\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "9r5S3sOSvXdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "words = [\"tupiro\", \"golabu\", \"bidaku\", \"padoti\"] # define the list\n",
        "\n",
        "print(\"Word list:\", words) # print the list to console to check the result\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "v9MVNJ41-5sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Great! Now, using the \"words\" list, we need to generate our experimental stream\n",
        "# comprising 180 words sampled at random, with no word appearing twice in a row.\n",
        "# Let's break down this problem a bit first to make it digestible, shall we?\n",
        "\n",
        "# Sample the first word from the list and name it \"first_word\"\n",
        "# This will be the starting word in our sequence\n",
        "# Pro Tips:\n",
        "# - Use a sampling method from the \"random\" library\n",
        "# - Set a seed for reproducibility\n",
        "# - The resulting word should be a string\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "mh-mbVVP4xvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import random  # Import the random module for random sampling functions\n",
        "\n",
        "random.seed(42)  # Set the seed to ensure reproducibility\n",
        "\n",
        "first_word = random.choice(words)  # Sample the first word\n",
        "print(\"First word:\", first_word)  # Print the first word to check the result\n",
        "\n",
        "second_word = random.choice(words)  # Sample an initial second word\n",
        "\n",
        "# Resample until the second word is different from the first\n",
        "while second_word == first_word:\n",
        "  second_word = random.choice(words)\n",
        "\n",
        "print(\"Second word:\", second_word)  # Print the second word to check it is different from the first\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "d0qDmbt6JRUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nice! Now we can go a step further and repeat the above process until we have a final\n",
        "# experimental stream of 180 words, with no word occurring twice in a row.\n",
        "\n",
        "# From the \"words\" list, sample 180 words with replacement (also ensuring no word occurs twice in a row),\n",
        "# and name the resulting object \"experimental_stream\"\n",
        "# Pro Tips:\n",
        "# - Initialise an empty list called \"experimental_stream\" to store the sampled words at each loop iteration\n",
        "# - Set up a for loop that repeats 180 times\n",
        "# - Use a sampling method with replacement that avoids consecutive word duplicates\n",
        "# - Set a seed for reproducibility\n",
        "# - The resulting sample should be a list called \"experimental_stream\" with 180 string elements\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "uC60GdgG_VsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "experimental_stream = []  # Initialise an empty list to store the sampled words\n",
        "\n",
        "random.seed(42)  # Set the seed to ensure reproducibility\n",
        "\n",
        "# Sample 180 words, ensuring no two consecutive words are the same\n",
        "for _ in range(180):\n",
        "    next_word = random.choice(words)  # Sample a word from the list\n",
        "    # Ensure the new word is not the same as the last word in the list\n",
        "    while experimental_stream and next_word == experimental_stream[-1]:\n",
        "        next_word = random.choice(words)  # Resample if the word is the same as the last one\n",
        "    experimental_stream.append(next_word)  # Add the word to the stream\n",
        "\n",
        "print(\"Experimental stream:\", experimental_stream)  # Print the entire list to the console\n",
        "print(\"Number of words in experimental stream:\", len(experimental_stream))  # Also verifying that it contains 180 elements\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "lrhSpe5jS5bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amazing! You have now created a list of words that can be presented\n",
        "# continuously without pauses, forming an experimental stream that follows Saffran et al.'s method.\n",
        "\n",
        "# Now, the final step is to generate 24 different experimental streams\n",
        "# (for our 24 models simulating the 24 infants).\n",
        "\n",
        "# Pro Tips:\n",
        "# - Use the code in the previous section to define a general function called \"generate_stream\", that can create an individual stream with 180 words\n",
        "# - Use a list comprehension to create 24 unique streams, each generated independently\n",
        "# - Remember to set a seed for reproducibility\n",
        "# - Each stream will be an element in a list called \"all_streams\"\n",
        "# - Thus, the resulting \"all_streams\" object should be a list of lists\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "Rc4g5favRVPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal the solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "# Define a function to generate a single experimental stream of 180 words\n",
        "def generate_stream(word_list, length):\n",
        "  \"\"\"\n",
        "  Generates a list of words with a specified number of words, ensuring no consecutive duplicates.\n",
        "  \n",
        "  Parameters:\n",
        "  - word_list: List of words to sample from\n",
        "  - length: Number of words in the generated stream\n",
        "  \n",
        "  Returns:\n",
        "  - A list containing the generated stream of words\n",
        "  \"\"\"\n",
        "  stream = []  # Initialise an empty list to store the sampled words\n",
        "  for _ in range(length):\n",
        "    next_word = random.choice(word_list)  # Sample a word from the list\n",
        "    # Ensure the new word is not the same as the last word in the list\n",
        "    while stream and next_word == stream[-1]:\n",
        "      next_word = random.choice(word_list)  # Resample if the word is the same as the last one\n",
        "    stream.append(next_word)  # Add the word to the stream\n",
        "  return stream\n",
        "\n",
        "random.seed(42)  # Set the seed to ensure reproducibility\n",
        "\n",
        "# Use a list comprehension to generate 24 different experimental streams\n",
        "all_streams = [generate_stream(words, 180) for _ in range(24)]\n",
        "\n",
        "# Print the number of streams and the length of each to verify correctness\n",
        "print(\"Number of experimental streams:\", len(all_streams))\n",
        "print(\"Number of words in each experimental stream:\", [len(stream) for stream in all_streams])\n",
        "\n",
        "# Print one of the streams to see its elements\n",
        "print(\"8th experimental stream:\", all_streams[7])\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "VnwYMft-bLtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Section Note\n",
        "# In the original study by Saffran et al., each word appeared exactly 45 times in the continuous stream.\n",
        "# As a homework task, try to implement this constraint: generate a stream where words are selected at random,\n",
        "# no word is repeated consecutively, and each word occurs exactly 45 times.\n",
        "\n",
        "# In our case, we can reasonably omit this constraint, because across our 24 different random streams,\n",
        "# each word should be repeated 45 times on average.\n",
        "\n",
        "# Here's some code to verify this:\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Compute word counts per stream\n",
        "stream_word_counts = [Counter(stream) for stream in all_streams]\n",
        "\n",
        "# For each word, gather its count across all 24 streams\n",
        "word_frequencies = {word: [] for word in words}\n",
        "for stream_count in stream_word_counts:\n",
        "    for word in words:\n",
        "        word_frequencies[word].append(stream_count[word])\n",
        "\n",
        "# Print average and standard deviation per word\n",
        "print(\"Average and standard deviation of word frequencies across the 24 streams:\\n\")\n",
        "for word in words:\n",
        "    counts = word_frequencies[word]\n",
        "    mean = np.mean(counts)\n",
        "    std = np.std(counts)\n",
        "    print(f\"{word}: M = {mean:.2f}, SD = {std:.2f}\")"
      ],
      "metadata": {
        "id": "gZSuoK954D5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2: Model Specification\n",
        "\n",
        "**Objective**: Define a computational model that calculates transitional probabilities between two co-occurring events (e.g., a pair of adjacent syllables).\n",
        "\n",
        "**Required Knowledge**:\n",
        "\n",
        "- Basic Concepts in Probability Theory (e.g., conditional probability and joint probability)\n",
        "- Basic Concepts in Statistical Analysis (e.g., frequency counts, statistical relationships between events)\n",
        "- Basic Understanding of Mathematical Notation and Equations\n",
        "\n",
        "**Define the Core Function that Calculates a Transitional Probability**:\n",
        "\n",
        "The transitional probability $P(B \\mid A)$ represents the probability of event $B$ occurring immediately after event $A$. It is calculated using the following formula:\n",
        "\n",
        "$$\n",
        "P(B \\mid A) = \\frac{C(A, B)}{C(A)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $C(A, B)$ is the count of times event $B$ follows event $A$.\n",
        "- $C(A)$ is the total count of event $A$ occurring.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9zHLe8A4jn0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3: Model Implementation\n",
        "\n",
        "**Objective**: Implement the transitional probability model to calculate the probability of one syllable following another within each continuous stream.\n",
        "\n",
        "**Required Python Knowledge**:\n",
        "\n",
        "- Function Definition and Parameter Use\n",
        "- Regular Expressions (`re` library) for Pattern Matching\n",
        "- `Counter` from the `collections` module for Frequency Counting\n",
        "- Using `zip` for Overlapping Pairs in Lists\n",
        "- Using `product` from the `itertools` library for Generating Combinations\n",
        "- List and Dictionary Manipulation\n",
        "- Loops and List Comprehensions\n",
        "- Console Output for Debugging (e.g., printing intermediate results to check frequency counts)\n",
        "\n",
        "**Tasks**:\n",
        "\n",
        "1. **Based on the model specification, construct a Python function that calculates the transitional probability between two adjacent syllables.**\n",
        "\n",
        "2. **Run the model for each of the 24 experimental streams.**\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Hnu0grcXo4k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Well done for getting this far! Now, let’s build a function to compute transitional probabilities\n",
        "# between two adjacent events, like two adjacent syllables.\n",
        "#\n",
        "# Pro tips:\n",
        "# - Define a function named \"transitional_probability\" that takes in two arguments:\n",
        "#   (1) count_AB: the number of times event B follows event A\n",
        "#   (2) count_A: the total number of occurrences of event A\n",
        "# - Within the function, ensure the denominator is not zero to avoid division errors.\n",
        "# - Return the calculated probability value as a float.\n",
        "#\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "0KIchPszY3jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "def transitional_probability(count_AB, count_A):\n",
        "    \"\"\"\n",
        "    Calculates the transitional probability of event B following event A.\n",
        "    \n",
        "    Parameters:\n",
        "    - count_AB (int): The count of occurrences where event B follows event A.\n",
        "    - count_A (int): The total count of occurrences of event A.\n",
        "    \n",
        "    Returns:\n",
        "    - float: The transitional probability P(B | A).\n",
        "    \"\"\"\n",
        "    if count_A == 0:\n",
        "        return 0  # Avoid division by zero if event A never occurs\n",
        "    return count_AB / count_A\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "gQ8yskylv5R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Great, we have a basic transitional probability function.\n",
        "# Consider it the core of our transitional probability model.\n",
        "\n",
        "# The basic function will need to work with two types of frequencies:\n",
        "# - frequencies AB (frequency of syllable B following syllable A)\n",
        "# - and frequencies A (frequency of syllable A)\n",
        "\n",
        "# Therefore, it will be easier to calculate all these frequencies in advance,\n",
        "# so they are ready for the model to access when needed.\n",
        "\n",
        "# For now, let's focus on the denominator of the transitional probability formula, as it’s easier to digest.\n",
        "# We'll take one of our 24 streams (e.g., stream 1) and compute the frequency\n",
        "# of each possible syllable in the stream.\n",
        "\n",
        "# We already know that the possible syllables are:\n",
        "syllables = [\"tu\", \"pi\", \"ro\", \"go\", \"la\", \"bu\", \"bi\", \"da\", \"ku\", \"pa\", \"do\", \"ti\"]\n",
        "\n",
        "# To check whether a syllable occurs in a word, we need to split the word\n",
        "# into its individual syllables, based on our list of possible syllables.\n",
        "\n",
        "# For example, let’s start by trying to split the word \"bidaku\" into its component syllables.\n",
        "word = \"bidaku\"\n",
        "\n",
        "# Pro tips:\n",
        "# - Step 1: Create a regular expression pattern to match any syllable in the \"syllables\" list.\n",
        "#   * Join the syllables with the '|' symbol to form a pattern that matches any one of these syllables.\n",
        "# - Step 2: Compile the regex pattern for efficient matching.\n",
        "# - Step 3: Use `re.findall()` with the compiled pattern to find all syllables in the word.\n",
        "#   * This returns a list of syllables from the word in sequence, based on the matches.\n",
        "# - Step 4: Verify that all parts of the word are valid syllables.\n",
        "#   * Reconstruct the word by joining the extracted syllables together.\n",
        "#   * If this reconstructed word differs from the original, raise a `ValueError` to indicate that\n",
        "#     unmatched parts exist in the word.\n",
        "# - Make sure to print the syllables found to confirm that the word is split correctly!\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "Mg-m5NB2wenM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import re  # Import the regular expressions module\n",
        "\n",
        "# Step 1: Create a regex pattern from the syllables list and compile it for efficient matching\n",
        "# Join all syllables with '|' to match any one of them in the word\n",
        "syllable_pattern = '|'.join(syllables)\n",
        "syllable_regex = re.compile(syllable_pattern)\n",
        "\n",
        "# Step 2: Use the regex to find all matching syllables in sequence within the word\n",
        "split_syllables = syllable_regex.findall(word)\n",
        "print(f\"Syllables in '{word}':\", split_syllables)\n",
        "\n",
        "# Step 3: Verify that all parts of the word are valid syllables by reconstructing the word\n",
        "# If the reconstructed word differs from the original, raise an error\n",
        "reconstructed_word = ''.join(split_syllables)\n",
        "if reconstructed_word != word:\n",
        "  raise ValueError(f\"The word '{word}' contains unmatched parts that do not correspond to valid syllables.\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "LeIP4lxv9q1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that you’ve figured out how to identify syllables in a word,\n",
        "# it’s easy to find all syllables in the experimental stream.\n",
        "# Simply treat the experimental stream as one very loooooong word.\n",
        "\n",
        "# Pro Tips:\n",
        "# - Take the first experimental stream, all_streams[0], and join all words into a single large string with no spaces, called \"collapsed_stream\".\n",
        "# - Apply the same code as above to find all syllables in this continuous string.\n",
        "# - The result should be a list of syllables called \"stream_syllables\".\n",
        "\n",
        "# - Magic Tip: Once, Richard Stallman appeared to me in a dream.\n",
        "#   Amidst lines of code and the hum of Linux coming to life, he leaned over and whispered:\n",
        "#   “When code repeats and grows a tad wild, wrap it in a function, and keep it styled!”\n",
        "# - Follow Stallman's advice and create a function called \"extract_syllables\"\n",
        "#   that can extract syllables from any given text using a provided syllables list.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "nefJ-Gaf2vrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import re  # Import the regular expressions module\n",
        "\n",
        "# Define a function to extract syllables from any given text using a provided syllables list\n",
        "def extract_syllables(text, syllables_list):\n",
        "    \"\"\"\n",
        "    Extracts syllables from the provided text based on a given list of syllables.\n",
        "    \n",
        "    Parameters:\n",
        "    - text (str): A string from which syllables are to be extracted.\n",
        "    - syllables_list (list): A list of valid syllables to match.\n",
        "    \n",
        "    Returns:\n",
        "    - list: A list of extracted syllables.\n",
        "    \n",
        "    Raises:\n",
        "    - ValueError: If there are unmatched parts in the text.\n",
        "    \"\"\"\n",
        "    # Step 1: Create and compile the regex pattern from the syllables list\n",
        "    syllable_pattern = '|'.join(syllables_list)\n",
        "    syllable_regex = re.compile(syllable_pattern)\n",
        "    \n",
        "    # Step 2: Use the regex to find all matching syllables in sequence\n",
        "    split_syllables = syllable_regex.findall(text)\n",
        "    \n",
        "    # Step 3: Verify that all parts of the text are valid syllables\n",
        "    reconstructed_text = ''.join(split_syllables)\n",
        "    if reconstructed_text != text:\n",
        "        raise ValueError(f\"The text '{text}' contains unmatched parts that do not correspond to valid syllables.\")\n",
        "    \n",
        "    return split_syllables\n",
        "\n",
        "collapsed_stream = ''.join(all_streams[0])  # Join the words in the first stream\n",
        "\n",
        "# Apply the function to the continuous stream\n",
        "stream_syllables = extract_syllables(collapsed_stream, syllables)\n",
        "print(f\"Syllables in the first experimental stream: {stream_syllables}\")\n",
        "print(f\"Number of syllables in the stream: {len(stream_syllables)}\")  # Verify that the syllables total 540 as per Saffran et al.'s design\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "1SLHHZgfGX8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have the object \"stream_syllables\", which contains all syllables in an experimental stream.\n",
        "# Next, we can count how many times each unique syllable appears in the stream.\n",
        "# This will give us all the frequencies we can use for the denominator of the transitional probability formula!\n",
        "\n",
        "# Pro tip:\n",
        "# - Use the \"Counter\" function from the \"collections\" package to compute the frequency of each unique syllable in \"stream_syllables\"\n",
        "# - The result should return a dictionary where each syllable is a key, and its frequency is the corresponding value.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "-CdagV2REtlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "from collections import Counter  # Import Counter for easy frequency counting\n",
        "\n",
        "# Count the frequency of each unique syllable in \"stream_syllables\"\n",
        "syllable_frequencies = Counter(stream_syllables)\n",
        "\n",
        "# Output the syllable frequencies\n",
        "print(\"Syllable frequencies in the stream:\")\n",
        "for syllable, frequency in syllable_frequencies.items():\n",
        "  print(f\"{syllable}: {frequency}\")\n",
        "\n",
        "# Optionally, verify the total count matches the length of stream_syllables\n",
        "print(f\"Total number of syllables counted: {sum(syllable_frequencies.values())}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "AlMvT2M2JlZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Great, you have extracted the frequencies needed to compute the denominator of the transitional probability.\n",
        "# In the two code chunks above, we defined the function extract_syllables() that extracts the syllables from a stream,\n",
        "# and then we counted the frequencies of the unique syllables in a separate step.\n",
        "# However, we can do everything together by creating a function called \"get_denominator_frequencies\" that takes\n",
        "# an experimental stream and a list of possible syllables, then finds the syllables in the stream and counts their frequencies.\n",
        "\n",
        "# Pro tip:\n",
        "# - You can extend the extract_syllables function by adding a step to count syllable frequencies with Counter,\n",
        "#   creating a new function, \"get_denominator_frequencies\", to handle both tasks in one go.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "r61e8LrNHpYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Define a function that collapses the stream, extracts syllables, and counts their frequencies\n",
        "def get_denominator_frequencies(stream_words, syllables_list):\n",
        "  \"\"\"\n",
        "  Collapses the stream of words into a continuous string, extracts syllables based on a given list,\n",
        "  and counts their frequencies.\n",
        "\n",
        "  Parameters:\n",
        "  - stream_words (list): A list of words forming the experimental stream.\n",
        "  - syllables_list (list): A list of valid syllables to match.\n",
        "\n",
        "  Returns:\n",
        "  - dict: A dictionary where each key is a syllable, and the value is its frequency.\n",
        "\n",
        "  Raises:\n",
        "  - ValueError: If there are unmatched parts in the collapsed stream.\n",
        "  \"\"\"\n",
        "  # Step 1: Collapse the stream into a single continuous string\n",
        "  collapsed_text = ''.join(stream_words)\n",
        "\n",
        "  # Step 2: Create and compile the regex pattern from the syllables list\n",
        "  syllable_pattern = '|'.join(syllables_list)\n",
        "  syllable_regex = re.compile(syllable_pattern)\n",
        "\n",
        "  # Step 3: Use the regex to find all matching syllables in sequence\n",
        "  split_syllables = syllable_regex.findall(collapsed_text)\n",
        "\n",
        "  # Step 4: Verify that all parts of the text are valid syllables\n",
        "  reconstructed_text = ''.join(split_syllables)\n",
        "  if reconstructed_text != collapsed_text:\n",
        "    raise ValueError(f\"The text '{collapsed_text}' contains unmatched parts that do not correspond to valid syllables.\")\n",
        "\n",
        "  # Step 5: Count the frequency of each syllable\n",
        "  syllable_frequencies = Counter(split_syllables)\n",
        "\n",
        "  return syllable_frequencies\n",
        "\n",
        "# Apply the new function to the first stream\n",
        "stream_syllable_frequencies = get_denominator_frequencies(all_streams[0], syllables)\n",
        "\n",
        "# Output the syllable frequencies\n",
        "print(\"Syllable frequencies in the stream:\")\n",
        "for syllable, frequency in stream_syllable_frequencies.items():\n",
        "  print(f\"{syllable}: {frequency}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "dkE3UYLCMt1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Denominator is done! Give yourself a pat on the back.\n",
        "\n",
        "# Now let's reflect on the numerator for a moment: \"given a syllable pair AB, the numerator is the count of times syllable B follows syllable A.\"\n",
        "# What does that mean? Well, it simply means we need to count the frequency of AB occurring! Simple as that.\n",
        "\n",
        "# I know what you're thinking now\n",
        "# \"Well, what if instead of a list of possible syllables, I compute a list of possible syllable pairs (like AB) and pass it to the\n",
        "# get_denominator_frequencies() function?\"\n",
        "# Maybe :D Let's try it\n",
        "\n",
        "# First, we need to find a way to compute all possible syllable pairs.\n",
        "\n",
        "# Pro tips:\n",
        "# - Think of each syllable pair as a two-syllable \"unit\" (like \"AB\") where A and B can each be any syllable from the syllables list.\n",
        "# - Use the \"syllables\" object and compute all possible two-syllable combinations. With 12 syllables, each one can be followed by any other,\n",
        "#   resulting in 12 x 12 = 144 combinations.\n",
        "# - The result should be a list of 144 strings, named \"syllable_pairs\".\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "K-mz2L8rM4s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "# Generate all possible pairs of syllables\n",
        "syllable_pairs = [''.join(pair) for pair in product(syllables, repeat=2)]\n",
        "\n",
        "# Print the generated pairs (optional)\n",
        "print(\"Possible syllable pairs:\", syllable_pairs)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "nIZOG3M3lyH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run the function using syllable_pairs as the reference list\n",
        "# WARNING: The function will throw an error :D Head to the next colab cell to find out why!\n",
        "get_denominator_frequencies([\"tupiro\"], syllable_pairs)"
      ],
      "metadata": {
        "id": "aUTWEKzVkbt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get a ValueError. Strange, the function at some point stops because it cannot find a valid syllable pair matching part of \"tupiro\".\n",
        "\n",
        "# When operating with syllables, for the word \"tupiro\" we need to find \"tu\", \"pi\", and \"ro\".\n",
        "# Instead, when operating with syllable pairs, these are the syllable pairs we need to find: \"tupi\", \"piro\".\n",
        "\n",
        "# the way our get_denominator_frequencies() function works now is that it tries to divide \"tupiro\" into\n",
        "# \"tupi\" and \"ro\" (so it fails because \"ro\" is not a valid syllable pair!).\n",
        "# Namely, it doesn't handle partially overlapping syllable pairs.\n",
        "\n",
        "# We need to create a modified version of the function (we are going to call it \"get_numerator_frequencies\") that considers\n",
        "# consecutive syllable pairs, where the final syllable of one pair always overlaps with the first syllable of the next adjacent one.\n",
        "\n",
        "# Let's try to find the consecutive overlapping pairs of syllables in \"tupiro\" (i.e., \"tupi\" and \"piro\")\n",
        "word = \"tupiro\"\n",
        "\n",
        "# Pro tip:\n",
        "# - Consider each consecutive pair of syllables as a bigram (e.g., (\"tu\", \"pi\") and (\"pi\", \"ro\")).\n",
        "# - Use the zip function to create overlapping pairs from the syllables list.\n",
        "# - Store these in a list of strings called \"syllable_pairs\".\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "icYmn5G_kn7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "# Define the list of syllables for the word \"tupiro\"\n",
        "tupiro_syllables = extract_syllables(word, syllables)\n",
        "\n",
        "# Create overlapping sy§llable pairs using zip\n",
        "syllable_pairs = [''.join(pair) for pair in zip(tupiro_syllables, tupiro_syllables[1:])]\n",
        "\n",
        "# Print the overlapping syllable pairs\n",
        "print(\"Overlapping syllable pairs in 'tupiro':\", syllable_pairs)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "fZOI4CUAs4n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Well done, you are becoming quite proficient with list comprehensions. They are very important in Python for creating compact, readable code.\n",
        "\n",
        "# Now that you've solved how to compute the overlapping syllable pairs, create the get_numerator_frequencies() function.\n",
        "# This function needs to extract consecutive overlapping syllable pairs from the experimental stream and count their frequencies.\n",
        "\n",
        "# Pro tip:\n",
        "# - To create overlapping pairs from a list of syllables, use `zip` with a list comprehension as you did above.\n",
        "#   This allows you to join each syllable with the next, ensuring that the last syllable of one pair overlaps\n",
        "#   with the first syllable of the next pair.\n",
        "# - Use `Counter` as you did in the \"get_denominator_frequencies\" function. It is an efficient way to count\n",
        "#   syllable pair occurrences once you’ve generated the list of pairs.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "gm2KphpytzcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def get_numerator_frequencies(stream_words, syllables_list):\n",
        "  \"\"\"\n",
        "  Extracts consecutive overlapping syllable pairs from the stream of words and counts their frequencies.\n",
        "  \n",
        "  Parameters:\n",
        "  - stream_words (list): A list of words forming the experimental stream.\n",
        "  - syllables_list (list): A list of valid syllables to match.\n",
        "  \n",
        "  Returns:\n",
        "  - dict: A dictionary where each key is a syllable pair (e.g., \"tupi\"), and the value is its frequency.\n",
        "  \n",
        "  Raises:\n",
        "  - ValueError: If there are unmatched parts in the syllable extraction process.\n",
        "  \"\"\"\n",
        "  # Step 1: Collapse the stream into a single continuous string\n",
        "  collapsed_text = ''.join(stream_words)\n",
        "  \n",
        "  # Step 2: Extract syllables using the existing extract_syllables function\n",
        "  stream_syllables = extract_syllables(collapsed_text, syllables_list)\n",
        "  \n",
        "  # Step 3: Create overlapping syllable pairs (bigrams)\n",
        "  syllable_pairs = [''.join(pair) for pair in zip(stream_syllables, stream_syllables[1:])]\n",
        "  \n",
        "  # Step 4: Count the frequency of each syllable pair\n",
        "  syllable_pair_frequencies = Counter(syllable_pairs)\n",
        "  \n",
        "  return syllable_pair_frequencies\n",
        "\n",
        "# Apply the new function to the first stream\n",
        "stream_syllable_pair_frequencies = get_numerator_frequencies(all_streams[0], syllables)\n",
        "\n",
        "# Output the syllable pair frequencies\n",
        "print(\"Syllable pair frequencies in the stream:\")\n",
        "for pair, frequency in stream_syllable_pair_frequencies.items():\n",
        "  print(f\"{pair}: {frequency}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "37OmkVL1wjtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Almost at the end of this section, well done! We now have everything we need to\n",
        "# compute the transitional probability of a syllable pair.\n",
        "\n",
        "# For example, let's try to compute the transitional probability of \"tupi\" (first syllable of \"tupiro\") in experimental stream 1.\n",
        "# First, save the frequencies.\n",
        "syllable_frequencies = get_denominator_frequencies(all_streams[0], syllables)\n",
        "syllable_pair_frequencies = get_numerator_frequencies(all_streams[0], syllables)\n",
        "\n",
        "# Now compute the syllable transitional probability\n",
        "tupi_first_syllable = extract_syllables(\"tupi\", syllables)[0]\n",
        "tupi_tp = transitional_probability(syllable_pair_frequencies[\"tupi\"], syllable_frequencies[tupi_first_syllable])\n",
        "\n",
        "print(\"TP of 'tupi' in experimental stream 1:\", tupi_tp)"
      ],
      "metadata": {
        "id": "_efVSN7e4LK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The final step is yours to tackle—no tips this time. Well, perhaps just one: the \"enumerate\" function might\n",
        "# come in handy here to loop through each experimental stream.\n",
        "# Your task is to compute the transitional probability (TP) for all consecutive syllable pairs in each experimental stream.\n",
        "# You now have all the tools you need to complete this!\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "XBazN6p36Lkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "# Initialise a dictionary to store the TP results for each stream\n",
        "all_tps = []\n",
        "\n",
        "# Loop through each experimental stream\n",
        "for stream_index, stream in enumerate(all_streams):\n",
        "    # Get the frequencies of individual syllables and syllable pairs for the current stream\n",
        "    syllable_frequencies = get_denominator_frequencies(stream, syllables)\n",
        "    syllable_pair_frequencies = get_numerator_frequencies(stream, syllables)\n",
        "\n",
        "    # Calculate the TP for each syllable pair in the current stream\n",
        "    stream_tp = {}\n",
        "    for pair in syllable_pair_frequencies:\n",
        "        first_syllable = extract_syllables(pair, syllables)[0]\n",
        "        tp = transitional_probability(syllable_pair_frequencies[pair], syllable_frequencies[first_syllable])\n",
        "        stream_tp[pair] = tp\n",
        "\n",
        "    # Append the TP results for this stream to the list\n",
        "    all_tps.append(stream_tp)\n",
        "\n",
        "# Output the TPs for each stream\n",
        "for stream_index, stream_tp in enumerate(all_tps):\n",
        "    print(f\"Transitional probabilities for experimental stream {stream_index + 1}:\")\n",
        "    for pair, tp in stream_tp.items():\n",
        "        print(f\"  TP of '{pair}': {tp}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "g00y5nePHGeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4: Analysis\n",
        "\n",
        "**Objectives**:\n",
        "- Demonstrate that TPs within words are higher than TPs between words within each continuous stream.\n",
        "- Expose each model to the test items (words and part-words) and compute a score that could serve as a proxy for \"fixation time\".\n",
        "- Compare the models' fixation times to those of infants from Saffran et al.'s study.\n",
        "\n",
        "**Required Python Knowledge**:\n",
        "\n",
        "- List and Dictionary Comprehension (to group and restructure results)\n",
        "- Aggregation with defaultdict (to merge values across streams)\n",
        "- Numerical Operations with numpy (mean, standard deviation, square root)\n",
        "- Confidence Intervals using Standard Error and Normal Approximation\n",
        "- Conditional Logic (e.g., handling division by zero)\n",
        "- Min–Max Normalisation (rescaling values to match a target range)\n",
        "- Bar Plotting with matplotlib.pyplot, including:\n",
        "    - Error bars (yerr and capsize)\n",
        "    - Custom figure size, labels, and legends\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "libxd-EXDtxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our first task in this section is to show that transitional probabilities (TPs) within words are higher than TPs between words.\n",
        "\n",
        "# Syllables within a word always occur together. For example, in the word \"tupiro\", \"pi\" always follows \"tu\".\n",
        "# In contrast, syllables at word boundaries do not consistently co-occur.\n",
        "# For instance, \"ro\" (from \"tupiro\") might be followed by \"go\" (from \"golabu\"),\n",
        "# but could just as easily be followed by \"bi\" (from \"bidaku\") or \"pa\" (from \"padoti\").\n",
        "# In other words, it’s harder to predict what comes next across word boundaries.\n",
        "\n",
        "# To demonstrate this, we’ll calculate:\n",
        "# 1. The average TP for syllable pairs *within* words\n",
        "# 2. The average TP for syllable pairs *between* words (i.e., at word boundaries)\n",
        "\n",
        "# We already have the TPs for all syllable pairs in each stream (stored in the all_tps object).\n",
        "# Let's start by finding out what the average TP is for each syllable pair across the 24 streams.\n",
        "\n",
        "# Pro tip:\n",
        "# - Use a defaultdict to gather all TP values for each syllable pair across the 24 streams.\n",
        "# - Once collected, use numpy’s mean function to compute the average TP for each pair.\n",
        "# - Store the result in a dictionary named avg_tps, which maps each syllable pair to its mean TP value.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "dmV5FgS7E-lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialise a defaultdict\n",
        "all_tps_across_streams = defaultdict(list)\n",
        "\n",
        "# Collect TPs for each syllable pair, from all 24 dictionaries\n",
        "for stream_tp in all_tps:\n",
        "    for pair, tp in stream_tp.items():\n",
        "        all_tps_across_streams[pair].append(tp)\n",
        "\n",
        "# For each syllable pair, compute the average TP across streams\n",
        "avg_tps = {pair: np.mean(tps) for pair, tps in all_tps_across_streams.items()}\n",
        "avg_tps\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "PwkFm94XO0zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brilliant — we now have the average transitional probabilities (TPs) across streams for each syllable pair.\n",
        "# The next step is to identify which syllable pairs occur *within* words and which occur *between* words.\n",
        "# Once we've grouped them accordingly, we can compute the average TP for each group and compare.\n",
        "\n",
        "# Pro tip:\n",
        "# - Use the `extract_syllables()` function to break each word into its syllables.\n",
        "# - Then use `zip` to generate overlapping syllable pairs within each word.\n",
        "# - Add all within-word syllable pairs to a set, then compare with the keys in avg_tps.\n",
        "# - Use list comprehensions or dictionary comprehensions to separate the within-word and between-word TPs.\n",
        "# - Use numpy to compute the mean and standard deviation (SD) of each group.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "XOG08GdYKPfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "words = [\"tupiro\", \"golabu\", \"bidaku\", \"padoti\"]\n",
        "syllables = [\"tu\", \"pi\", \"ro\", \"go\", \"la\", \"bu\", \"bi\", \"da\", \"ku\", \"pa\", \"do\", \"ti\"]\n",
        "\n",
        "# Generate all within-word syllable pairs\n",
        "within_word_pairs = set()\n",
        "for word in words:\n",
        "    syll_list = extract_syllables(word, syllables)\n",
        "    within_pairs = [''.join(pair) for pair in zip(syll_list, syll_list[1:])]\n",
        "    within_word_pairs.update(within_pairs)\n",
        "\n",
        "# Now use the above to split the TPs into two groups\n",
        "within_word_tps = {pair: tp for pair, tp in avg_tps.items() if pair in within_word_pairs}\n",
        "between_word_tps = {pair: tp for pair, tp in avg_tps.items() if pair not in within_word_pairs}\n",
        "\n",
        "# Compute average and standard deviation for each group\n",
        "import numpy as np\n",
        "\n",
        "within_values = list(within_word_tps.values())\n",
        "between_values = list(between_word_tps.values())\n",
        "\n",
        "within_mean = np.mean(within_values)\n",
        "within_sd = np.std(within_values)\n",
        "between_mean = np.mean(between_values)\n",
        "between_sd = np.std(between_values)\n",
        "\n",
        "# Print results in a clean format\n",
        "print(\"Average Transitional Probability (TP)\")\n",
        "print(\"-------------------------------------\")\n",
        "print(f\"Within-word  : Mean = {within_mean:.3f}, SD = {within_sd:.3f}\")\n",
        "print(f\"Between-word : Mean = {between_mean:.3f}, SD = {between_sd:.3f}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "HSzwsb8_Rrl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Well done — the first task in this section is completed. We've shown that transitional probabilities (TPs) between words are relatively low.\n",
        "\n",
        "# One way to think about this is that, once we’ve heard a word, it’s difficult to predict what the next syllable will be\n",
        "# (i.e., the first syllable of the upcoming word). However, once we hear the first syllable of a new word,\n",
        "# the rest of the syllables in that word become easier to predict — at least for a statistical model.\n",
        "\n",
        "# Saffran’s study showed that infants were also sensitive to these probability differences and used such knowledge at test.\n",
        "# Let’s now examine whether our transitional probability model shows similar results when exposed to the test items.\n",
        "# If our models behave like the infants in the original study, they should assign higher\n",
        "# transitional probabilities to the words than to the part-words.\n",
        "\n",
        "# Our next task is to expose each of our 24 transitional probability models to the test items used in Saffran et al.’s study.\n",
        "# Go back to the Introduction if you need a reminder of what the \"part-words\" are.\n",
        "\n",
        "# First, define the words and part-words\n",
        "words = [\"tupiro\", \"golabu\", \"bidaku\", \"padoti\"]\n",
        "partwords = [\"rogola\", \"bubida\", \"kupado\", \"titupi\"]\n",
        "\n",
        "# We'll calculate the average syllable pair TP for each test item (i.e., each word or part-word),\n",
        "# and then compute the mean TP for the four words and four part-words in each model.\n",
        "\n",
        "# Pro tip:\n",
        "# - Use the `extract_syllables()` function to break down each test item into its syllables.\n",
        "# - Use `zip` to construct syllable pairs from these syllables.\n",
        "# - For each syllable pair, retrieve its TP from the model's TP dictionary (`stream_tp`).\n",
        "# - If a pair is missing from the dictionary, use 0 as a default (this is a reasonable fallback for unseen transitions).\n",
        "# - Repeat this for all 24 models, and store the results in separate lists for words and part-words.\n",
        "\n",
        "# INSERT CODE BELOW\n"
      ],
      "metadata": {
        "id": "JdRmZzwvTJtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define function to compute average syllable pair TP for an item, given a stream tp dictionary\n",
        "def calculate_avg_tp(word, stream_tp):\n",
        "    syllables_in_word = extract_syllables(word, syllables)\n",
        "    pairs = [''.join(pair) for pair in zip(syllables_in_word, syllables_in_word[1:])]\n",
        "    avg_tp = np.mean([stream_tp.get(pair, 0) for pair in pairs])  # Use 0 if pair is missing\n",
        "    return avg_tp\n",
        "\n",
        "# Store average TPs across models\n",
        "word_tps = []\n",
        "partword_tps = []\n",
        "\n",
        "# Compute mean TPs across all experimental streams\n",
        "for stream_tp in all_tps:\n",
        "    word_avg_tps = [calculate_avg_tp(word, stream_tp) for word in words]\n",
        "    partword_avg_tps = [calculate_avg_tp(partword, stream_tp) for partword in partwords]\n",
        "\n",
        "    word_tps.append(np.mean(word_avg_tps))\n",
        "    partword_tps.append(np.mean(partword_avg_tps))\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Model response summary:\")\n",
        "print(\"------------------------\")\n",
        "print(f\"Words     : Mean TP = {np.mean(word_tps):.3f}, SD = {np.std(word_tps):.3f}\")\n",
        "print(f\"Part-words: Mean TP = {np.mean(partword_tps):.3f}, SD = {np.std(partword_tps):.3f}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "UcsPJG9TbEjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nice! Our models seem to perform like the infants in the original study — they assign higher transitional probabilities (TPs)\n",
        "# to the words than to the part-words. This mirrors the infants’ longer fixation times for part-words, which suggests that our\n",
        "# simple TP model might indeed capture an underlying mechanism that supports statistical learning in early language acquisition.\n",
        "\n",
        "# This way of presenting the model performance is often used in computational modelling studies (e.g., French et al., 2011, with the TRACX model)\n",
        "# to examine the idea that TPs may explain observed human behaviour.\n",
        "\n",
        "# Now, let’s take it a step further.\n",
        "\n",
        "# Can we directly compare our model’s output to the fixation times measured in infants?\n",
        "# Well, not yet — because our model’s scores (i.e., raw transitional probabilities) are not on the same scale as seconds of looking time.\n",
        "\n",
        "# One way to make them comparable is to rescale the model’s TP scores so that they fall into a similar numerical range as the\n",
        "# fixation times reported by Saffran et al. This allows us to visually assess whether the relative difference between conditions (words vs. part-words)\n",
        "# in the model mirrors the relative difference seen in the behavioural data.\n",
        "\n",
        "# But how should we do that?\n",
        "\n",
        "# Step 1: First, recall that lower TPs correspond to greater prediction uncertainty / lower familiarity. In the context of infant studies, greater uncertainty\n",
        "# might lead to longer looking times (a novelty preference effect). To make our model reflect this intuition, we invert the TP scores:\n",
        "#     high TP → low fixation\n",
        "#     low TP → high fixation\n",
        "\n",
        "# Use a small epsilon close to 0 to avoid divisions by zero\n",
        "epsilon = 1e-8\n",
        "fixation_word_tps = [1/(tp + epsilon) for tp in word_tps]\n",
        "fixation_partword_tps = [1/(tp + epsilon) for tp in partword_tps]\n",
        "\n",
        "# Step 2: Rescale the individual inverted scores to the empirical fixation time range\n",
        "# Saffran et al. (1996) fixation time data\n",
        "saffran_means = [7.97, 8.85]  # Mean fixation times (in seconds)\n",
        "saffran_ci = [0.41 * 1.96, 0.45 * 1.96]     # 95% CIs\n",
        "\n",
        "# Define the min and max of the empirical range\n",
        "min_fixation = min(saffran_means) - min(saffran_ci)\n",
        "max_fixation = max(saffran_means) + max(saffran_ci)\n",
        "\n",
        "# Get the min and max of the model’s inverted values\n",
        "all_fixation_model_scores = fixation_word_tps + fixation_partword_tps\n",
        "min_model = min(all_fixation_model_scores)\n",
        "max_model = max(all_fixation_model_scores)\n",
        "\n",
        "# Apply min–max scaling to the individual values\n",
        "scaled_fixation_word_tps = [min_fixation + (x - min_model) * (max_fixation - min_fixation) / (max_model - min_model) for x in fixation_word_tps]\n",
        "scaled_fixation_partword_tps = [min_fixation + (x - min_model) * (max_fixation - min_fixation) / (max_model - min_model) for x in fixation_partword_tps]\n",
        "\n",
        "# Step 3: Compute mean and 95% confidence intervals of the rescaled values\n",
        "def compute_mean_and_ci(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Returns the mean and half-width of the confidence interval for a list of values.\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std_err = np.std(data, ddof=1) / np.sqrt(len(data))\n",
        "    margin = std_err * 1.96  # For 95% confidence assuming normality\n",
        "    return mean, margin\n",
        "\n",
        "scaled_mean_word, scaled_ci_word = compute_mean_and_ci(scaled_fixation_word_tps)\n",
        "scaled_mean_partword, scaled_ci_partword = compute_mean_and_ci(scaled_fixation_partword_tps)\n",
        "\n",
        "scaled_model_means = [scaled_mean_word, scaled_mean_partword]\n",
        "scaled_model_ci = [scaled_ci_word, scaled_ci_partword]\n",
        "\n",
        "# Plotting the results\n",
        "labels = ['Words', 'Part-words']\n",
        "width = 0.35\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "bars1 = ax.bar(x - width/2, saffran_means, width, yerr=saffran_ci, capsize=5, label=\"Saffran et al. (1996)\", alpha=0.7)\n",
        "bars2 = ax.bar(x + width/2, scaled_model_means, width, yerr=scaled_model_ci, capsize=5, label=\"Scaled Model TP Scores\", alpha=0.7)\n",
        "\n",
        "# Add labels and legend\n",
        "ax.set_ylabel('Mean Fixation Time (s) / Scaled Model Scores')\n",
        "ax.set_title(\"Comparison of Saffran et al.'s Results with Scaled Model TP Scores\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YTR-yPd-EPmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎉🎉🎉\n",
        "\n",
        "Congratulations on completing the first part of the course!\n",
        "You’ve successfully built a computational model that simulates how transitional probabilities might support word segmentation in infancy, and you’ve compared your model’s performance to real experimental data from a classic study in cognitive science. This is an impressive achievement — you’re now thinking like a computational modeller!\n",
        "\n",
        "⸻\n",
        "\n",
        "🧠 Final Point for Reflection\n",
        "\n",
        "Let’s take a closer look at the visual comparison we generated between the model and the infants’ performance. Why are the confidence intervals for the model so narrow? Why might infants' scores be more variable than a model?\n",
        "\n",
        "⸻\n",
        "\n",
        "This is a perfect point to pause and celebrate what you’ve achieved. 🥳\n",
        "\n",
        "In the next part of the course, you’ll have the opportunity to run simulations on conversational corpus data, and use a computational model implementing a chunking-based learning mechanism.\n",
        "\n",
        "See you soon!\n"
      ],
      "metadata": {
        "id": "ANV3stNMss5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 Bonus Track\n",
        "\n",
        "*Great work! now enjoy your musical reward!* 🎶\n",
        "\n",
        "<audio controls>\n",
        "  <source src=\"https://drive.google.com/uc?export=download&id=17kEucr1p_VaI5jLGPidrDtIldxw95rCn\" type=\"audio/mpeg\">\n",
        "  Your browser does not support the audio element.\n",
        "</audio>\n",
        "\n",
        " *by ChatGPT + SunoAI*"
      ],
      "metadata": {
        "id": "JqQ9dVGI5AHe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ha_8Pjz1vUgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}