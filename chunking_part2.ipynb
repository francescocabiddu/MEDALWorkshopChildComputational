{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "When using this notebook in **Playground mode**:\n",
        "- You can **run all cells** and see the results.\n",
        "- **Any changes you make will not be saved** after you close the notebook.\n",
        "\n",
        "If you’d like to make changes and save your work, please create a copy of this notebook in your own Google Drive:\n",
        "1. Go to **File > Save a copy in Drive**.\n",
        "2. This will create an editable version in your Drive, where you can modify and save your changes.\n",
        "\n",
        "If you ever want to reset the notebook to its original state, use this [link](https://colab.research.google.com/drive/1HPSSzWEaAmf9P-5UyoTcI1QbKgf2xvdJ#scrollTo=EmVl9xi2bRKC&forceEdit=true&sandboxMode=true) to reopen it in Playground mode.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EmVl9xi2bRKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge: Using a Chunking-Based Model to Simulate Early Vocabulary Growth  \n",
        "\n",
        "In this challenge, you will **use a computational model of “chunking”** – the idea that events which frequently co-occur in the environment gradually become represented as unified units (chunks), leading to more fluent processing over time. We implement this idea by simulating the theoretical proposal that children do not learn each word from scratch, but acquire vocabulary by re-using an ever-growing inventory of sub-lexical and lexical chunks they have already mastered (Jones & Rowland, [2017](https://doi.org/10.1016/j.cogpsych.2017.07.002)).  \n",
        "\n",
        "For example, having already learned the sub-lexical sequence /mˈɐ/ in /mˈɐmi/ (mummy) may facilitate the later learning of words such as /mˈɐŋki/ (monkey), /mˈɐd/ (mud), /mˈɐndˌeɪ/ (Monday), or /mˈɐtʃ/ (much), due to familiarity with at least part of their phonological forms.  \n",
        "\n",
        "<br>\n",
        "\n",
        "> **The Importance of Chunking in Cognitive Science**  \n",
        "> * Infants, children, and adults spontaneously group items that frequently co-occur in the input (e.g., Jones et al., [2020](https://doi.org/10.1016/j.cognition.2020.104200); Miller, [1956](https://doi.org/10.1037/h0043158); Slone & Johnson, [2018](https://doi.org/10.1016/j.cognition.2018.05.016)).  \n",
        "> * Computational models of chunking have shown that a small set of psychologically plausible learning mechanisms can account for core findings in domains such as expert memory, problem solving, and verbal learning (e.g., Gobet et al., [2015](https://doi.org/10.3389/fpsyg.2015.01785)) – and, crucially for us, in **child language acquisition** (e.g., Cabiddu et al., [2023](https://doi.org/10.1111/lang.12559); Jessop et al., [2025](https://dx.doi.org/10.1037/rev0000564); Jones et al., [2021](https://doi.org/10.1016/j.jml.2021.104232)).\n",
        "\n",
        "<br>\n",
        "\n",
        "## What you’ll do in this notebook\n",
        "1. **Familiarise yourself** with a simple chunking learner that processes child-directed utterances one at a time, storing new chunks formed from adjacent units that have just been encoded.  \n",
        "2. **Compare the model’s learning to real child learning.** Given a parent–child pair, you will evaluate how well the model vocabulary trained on the parent’s speech reflects the child’s productive vocabulary.  \n",
        "3. **Manipulate the input** to examine how the *quantity* and *diversity* of parental speech affect vocabulary growth. This will involve a pure simulation experiment designed to provide proof-of-principle evidence and test hypotheses about the role of different input characteristics in early word learning!\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "d_3sq34cd6Fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background  \n",
        "\n",
        "Understanding how infants and toddlers acquire their vocabularies is a central puzzle in language acquisition. **Chunking-based learning** offers a parsimonious answer: children store and re-use recurring fragments of phonological material (chunks), gradually assembling them into the larger units we recognise as words.  \n",
        "\n",
        "### Why chunking matters for vocabulary growth  \n",
        "* Chunking provides a **mechanistic link** between perception and learning.  \n",
        "* An expanding inventory of chunks acts as **linguistic scaffolding**: newly heard words often contain familiar sub-chunks, allowing the child to encode – and learn – those words more efficiently.    \n",
        "\n",
        "### Quantity vs diversity of the input  \n",
        "Researchers typically distinguish two key properties of children’s language experience:\n",
        "\n",
        "-  **Quantity** ➜ Total word **tokens** (all child-directed input words, repetitions included) ➜ “How much speech does the child hear?”\n",
        "-  **Quality / Diversity** ➜ Number of unique word **types** ➜ “How many *different* words does the child hear?”\n",
        "\n",
        "In naturalistic data these measures are **highly correlated** – talkative caregivers also tend to use a wider range of words – making it difficult to tease their effects apart.\n",
        "\n",
        "### Insights from Jones & Rowland (2017)  \n",
        "Jones and Rowland's study resolved this confound with simulations that **independently manipulated** quantity and lexical diversity. Four headline findings emerged:\n",
        "\n",
        "1. **Early advantage for quantity:** when the model’s lexicon is tiny, simply hearing more tokens speeds initial learning.  \n",
        "2. **Later advantage for diversity:** once a basic chunk repertoire is in place, exposure to *diverse* vocabulary yields steeper, more sustained vocabulary growth.  \n",
        "3. **Broader cognitive pay-offs:** models trained on diverse input outperform quantity-matched peers on non-word repetition, sentence recall, and the ability to learn completely novel words.  \n",
        "4. **Converging child data:** in English-speaking two- to three-year-olds, caregiver lexical diversity – not quantity – was the stronger predictor of later productive vocabulary.\n",
        "\n",
        "### Computational model used in the simulations  \n",
        "\n",
        "**CLASSIC** – *Chunking Lexical And Sub-lexical Sequence Incremental Computation*.  \n",
        "\n",
        "* **Input format:** word-delimited phonemic transcriptions of child-directed utterances.  \n",
        "* **Learning mechanism:** whenever two adjacent chunks are encoded, the model stores their concatenation as a new chunk, gradually building a hierarchy from phonemes → sub-lexical sequences → words → multi-word units.  \n",
        "* **Processing constraint:** on each utterance the model can access, on average, 4–5 chunks, with a recency bias favouring utterance-final material – mimicking children’s limited online processing capacity.  \n",
        "* **Outcome measures:** number and size of learned chunks, rate of learning novel words.\n",
        "\n",
        "### Child-directed speech corpus used  \n",
        "\n",
        "**Manchester corpus** (Theakston et al., [2001](https://talkbank.org/childes/access/Eng-UK/Manchester.html)):  \n",
        "\n",
        "* **Participants:** 12 English-learning children (initial age 1;10–2;0) and their primary caregivers.  \n",
        "* **Sampling schedule:** 34 one-hour recordings per dyad, roughly every three weeks over a twelve-month period.  \n",
        "* **Scale:** ≈ 410 000 caregiver word tokens, ≈ 12 000 word types, enabling fine-grained manipulation of quantity and diversity in the simulations.\n",
        "\n",
        "### What this means for our challenge  \n",
        "\n",
        "In this second part of the course you will implement a simplified chunking learner inspired by CLASSIC, feed it child-directed utterances (we'll use just one cargiver-child dyad for this course), and explore:\n",
        "\n",
        "* How the model’s vocabulary growth compares with the actual child vocabulary drawn from the corpus.  \n",
        "* How growth curves shift under **input manipulated in quantity or diversity**.  \n",
        "* Whether the same trade-off found by Jones & Rowland – early benefits of sheer exposure versus later benefits of lexical richness – emerges in your runs.  \n",
        "* Practical implications: could boosting the *variety* of words in a child’s daily input accelerate vocabulary learning more than simply talking more?\n",
        "\n",
        "By the end you should have an intuition for **how linguistic input structure shapes learning**, and hands-on experience manipulating corpora to isolate these effects!\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sKGFSnt8oFsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Quick Look at Jones & Rowland's Findings\n",
        "\n",
        "The image below shows the cumulative number of unique words learned by the **CLASSIC** computational model after being trained on three types of maternal input from the *Manchester Corpus*:\n",
        "\n",
        "- **Mother**: the original unaltered input  \n",
        "- **Diversity**: input modified to include a wider range of unique words  \n",
        "- **Quantity**: input with the same set of words as *Maternal*, but repeated more often  \n",
        "\n",
        "As shown in the graph, the model trained on the *Quantity* input has an early advantage. However, the model exposed to the *Diversity* input catches up and ultimately acquires a larger vocabulary over time.\n",
        "\n",
        "The authors suggest that highly repetitive input is helpful in the early stages of word learning, as it helps build a foundational set of sublexical chunks. Once these basic building blocks are in place, more diverse input allows the learner to rapidly expand their vocabulary.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1u0kXuNeLekO6p3NBjzQgxmEVjIEZR-mX\" width=\"450px\" border=\"2px\">\n",
        "\n",
        "*Figure from Jones & Rowland (2017)*\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8_UxOqZFIVkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: A simplified version of CLASSIC\n",
        "\n",
        "In this challenge, we provide you with a basic chunking model inspired by CLASSIC. This part of the course focuses more on experimenting with the model’s input rather than building its architecture.\n",
        "\n",
        "### Overview of Mini-CLASSIC:\n",
        "\n",
        "**Unconstrained Chunking Mechanism**:  \n",
        "The model uses the same unconstrained chunking mechanism as the original CLASSIC. It takes an utterance transcribed into phonemes—such as (d uː) (j uː) (w ɒ n ə) (m æ ʃ) (s ɐ m) (p ə t ɑː t əʊ z) / (do) (you) (wanna) (mash) (some) (potatoes)—and processes it from left to right by encoding it using available chunks from its lexicon.  \n",
        "\n",
        "If this is the first utterance the model sees, its lexicon only contains the basic phonemes of the language. It then forms new biphone chunks from adjacent phonemes:  \n",
        "d uː – uː j – j uː – uː w – w ɒ – ɒ n – n ə – ə m – m æ – æ ʃ – ʃ s – s ɐ – ɐ m – m p – p ə – ə t – t ɑː – ɑː t – t əʊ – əʊ z.\n",
        "\n",
        "If you look closely, the model has already reached a word-level representation for the words *do* and *you*. At this point, we say the model has learned a vocabulary of two distinct words!\n",
        "\n",
        "These newly learned chunks can then be reused to encode future utterances using longer chunks instead of individual phonemes. The use of longer chunks during encoding is a proxy for processing efficiency—the fewer the chunks used to encode a word or utterance, the easier it is to process and learn.\n",
        "\n",
        "For example, if the next utterance is (j uː) (g ɒ t) (s ɐ m) (p ə t ɑː t əʊ z) / (you) (got) (some) (potatoes), the model may encode it using the previously learned biphone chunks:  \n",
        "(juː) (g) (ɒ) (t) (sɐ) (mp) (ət) (ɑːt) (əʊz).\n",
        "\n",
        "After encoding, the model forms new chunks from adjacent elements:  \n",
        "j uː g – g ɒ – ɒ t – t sɐ – sɐ mp – mp ət – ət ɑːt – ɑːt əʊz.\n",
        "\n",
        "These chunks are now longer—some include 3 or 4 phonemes! Notice the chunk *sɐmp*, which crosses a word boundary and contains the word *sɐm* (some). At this point, we add *some* to the lexicon, meaning the model now has a vocabulary of three words.\n",
        "\n",
        "**Learning Rate**:  \n",
        "The model includes a learning rate parameter ranging from 0 to 1. It controls the likelihood of forming a new chunk from two adjacent elements. A learning rate of 1 means all possible adjacent sequences are chunked, while a rate of 0 means none are.\n",
        "\n",
        "**Recency Bias**:  \n",
        "We’ve also added a recency bias parameter, which makes the model more likely to learn chunks from the end of an utterance. This bias is implemented in Jones and Rowland’s simulations, under the assumption that children often pay more attention to the ends of utterances. A recency bias of 0 means no bias, while higher values increasingly favour the final chunks. At the extreme, the model may only learn the last chunk of each utterance.\n",
        "\n",
        "### Play with Mini-CLASSIC:\n",
        "\n",
        "In the following code cell, you’ll find a function that defines the entire Mini-CLASSIC model. Run the code cell to load the model. Next, try running the model on three consecutive utterances to see what chunks and words it learns. You can also experiment with the learning rate and recency bias parameters to observe how they affect the model’s behaviour.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "DyOBIGLRMEyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def run_chunking_model(input_utterances, learning_rate=1.0, final_bias_strength = 0,set_seed=None):\n",
        "    \"\"\"\n",
        "    Given a list of input utterances,\n",
        "    this function runs an incremental chunking learning model inspired by CLASSIC (Jones & Rowland, 2017).\n",
        "\n",
        "    The model:\n",
        "    - Parses each utterance into basic units (e.g., phonemes).\n",
        "    - Encodes incrementally by matching known chunks in the current lexicon.\n",
        "    - Learns new chunks by combining adjacent encoded elements and adds them to the lexicon.\n",
        "    - Tracks, at each step, which new chunks and input words were learned.\n",
        "    - Applies a learning probability (learning_rate) and optionally biases learning toward utterance-final chunks.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    input_utterances : list of str\n",
        "        Input utterances formatted as strings like \"(h e l l o) (d a d d y)\".\n",
        "\n",
        "    learning_rate : float, optional (default=1.0)\n",
        "        Probability of learning a new chunk when encountered. 1.0 means always learn; 0.0 means never.\n",
        "\n",
        "    final_bias_strength : float, optional (default=0)\n",
        "        Strength of positional bias toward learning later chunks in the utterance.\n",
        "        - 0 disables position-based bias (uniform probability).\n",
        "        - Values >1.0 increase preference for utterance-final chunks.\n",
        "        - Values <1.0 reduce the bias effect.\n",
        "        - 1.0 applies a linear bias (proportional to position).\n",
        "\n",
        "\n",
        "    set_seed : int or None, optional (default=None)\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    df : pandas.DataFrame\n",
        "        A DataFrame with the following columns:\n",
        "        - 'cds_utterance_id': utterance ID number\n",
        "        - 'utterance_phonemes': the original input utterance\n",
        "        - 'encoded_input': encoded version of the utterance using previously learned chunks\n",
        "        - 'new_chunks': list of new chunks learned at this utterance\n",
        "        - 'new_words': list of new words learned after processing the current utterance\n",
        "    \"\"\"\n",
        "    # ── parameter validation ───────────────────────────────\n",
        "    if not (0.0 <= learning_rate <= 1.0):\n",
        "        raise ValueError(\"learning_rate must be between 0 and 1 (inclusive).\")\n",
        "    if final_bias_strength < 0:\n",
        "        raise ValueError(\"final_bias_strength must be zero or positive.\")\n",
        "\n",
        "    # set seed\n",
        "    if set_seed is not None:\n",
        "        np.random.seed(set_seed)\n",
        "\n",
        "    # progress-bar setup\n",
        "    total_utts = len(input_utterances)\n",
        "    checkpoints = {\n",
        "        int(total_utts * 0.20): \"20 %\",\n",
        "        int(total_utts * 0.40): \"40 %\",\n",
        "        int(total_utts * 0.60): \"60 %\",\n",
        "        int(total_utts * 0.80): \"80 %\",\n",
        "    }\n",
        "    start_time = time.time()\n",
        "\n",
        "    utterances_orig = input_utterances\n",
        "\n",
        "    utterances = [\n",
        "        \" \".join(re.findall(r\"\\((.*?)\\)\", utt))\n",
        "        for utt in utterances_orig\n",
        "    ]\n",
        "\n",
        "    # Check for empty utterances and raise an error if any are found\n",
        "    empty_utterance_count = sum(utterance.strip() == \"\" for utterance in utterances)\n",
        "    if empty_utterance_count > 0:\n",
        "        raise ValueError(f\"Input contains {empty_utterance_count} empty utterance(s). Please remove or fix before running the model.\")\n",
        "\n",
        "    # create list of unsegmented sentences for final printing\n",
        "    utterances_base = [re.sub(\"_\", \"\", utterance) for utterance in utterances]\n",
        "\n",
        "    # create a list of unique units (phonemic or syllabic) to be converted into unique characters\n",
        "    # (i.e., characters whose Unicode code point is the integer i [see built-in function chr()])\n",
        "    unique_chars = list(set(list(np.concatenate([utterances[i].split() for i in range(len(utterances))]))))\n",
        "    unique_chars += \"|\" # add separator character for final segmentation printing\n",
        "\n",
        "    # generate unique characters to convert each different phonemic/syllabic unit\n",
        "    chars = [c for c in map(chr, range(0, len(unique_chars))) if len(c) == 1]\n",
        "\n",
        "    # create look-up dictionary to prepare input for algorithm\n",
        "    chars_in = dict(zip(unique_chars, chars))\n",
        "\n",
        "    # create look-up dictionary to convert algorithm output back to original phonemic/syllabic transcription\n",
        "    chars_out = dict(zip(chars, unique_chars))\n",
        "\n",
        "    # convert input utterances into strings of unique character units\n",
        "    utt_conversion = [utterance.split() for utterance in utterances]\n",
        "    utterances = [\"\".join([chars_in[unit] for unit in utterance]) for utterance in utt_conversion]\n",
        "\n",
        "    # define boundary separator\n",
        "    separator = chars_in[\"|\"]\n",
        "\n",
        "    # initialise variables\n",
        "\n",
        "    lexicon_nopause = [chars_in[unique_char] for unique_char in unique_chars if unique_char != \"|\"]\n",
        "    lexicon_nopause = dict(zip(lexicon_nopause, lexicon_nopause))\n",
        "\n",
        "    final_segmentation = []\n",
        "    lexicon_by_utterance = []\n",
        "\n",
        "    regex_separator_1 = \"\".join([\"^[\", separator, \"]\"])\n",
        "    regex_separator_2 = \"\".join([\"[\", separator, \"]{2,}\"])\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in range(len(utterances)): # for each utterance\n",
        "        utt = utterances[i]\n",
        "        final_seg = []\n",
        "\n",
        "        while len(utt) > 0:\n",
        "            # all adjacent combinations ordered by decreasing length\n",
        "            combinations = reversed([utt[0: z] for z in range(0 + 1, len(utt) + 1)])\n",
        "            chunk_found = \"\"\n",
        "\n",
        "            while chunk_found == \"\":\n",
        "                combination = next(combinations) # for each combination\n",
        "                # note, a chunk will always be found as a phoneme/syllable\n",
        "                # this happens because the lexicon has the phonemes/syllables basic units initialised\n",
        "\n",
        "                try:\n",
        "                    # look for a combination in the lexicon\n",
        "                    chunk_found = lexicon_nopause[combination]\n",
        "                except KeyError:\n",
        "                    # otherwise pass\n",
        "                    pass\n",
        "\n",
        "            # store chunk found\n",
        "            final_seg += [chunk_found]\n",
        "\n",
        "            # delete part of utterance that was matched by a chunk\n",
        "            utt = utt[len(chunk_found):]\n",
        "\n",
        "        chunks = []\n",
        "        # create new chunks from recognised segments\n",
        "        if len(final_seg) > 1:\n",
        "            for j in range(len(final_seg) - 1):\n",
        "                chunks += [\"\".join([final_seg[j], final_seg[j + 1]])]\n",
        "        else:\n",
        "            chunks += [final_seg[0]]\n",
        "\n",
        "        new_chunks = []\n",
        "\n",
        "        for j, chunk in enumerate(chunks): # add chunks to the lexicon\n",
        "            if chunk not in lexicon_nopause:\n",
        "                position_bias = ((j + 1) / len(chunks)) ** final_bias_strength # positional bias\n",
        "                if np.random.rand() < (learning_rate * position_bias): # learning rate\n",
        "                    lexicon_nopause[chunk] = chunk\n",
        "                    new_chunks.append(chunk)\n",
        "\n",
        "        # store only newly learned chunks for this utterance\n",
        "        lexicon_by_utterance.append(new_chunks)\n",
        "\n",
        "        # join all the recognised segmentes into a single string\n",
        "        final_seg_joined = re.sub(regex_separator_1,\n",
        "                                \"\",\n",
        "                                re.sub(regex_separator_2,\n",
        "                                        separator,\n",
        "                                        separator.join(final_seg)))\n",
        "\n",
        "        # add utterance segmentation to list of segmented utterances\n",
        "        final_segmentation += [final_seg_joined]\n",
        "\n",
        "        # ───── print progress every 20 % ─────\n",
        "        if i in checkpoints:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"[{checkpoints[i]}]   processed {i+1:,} / {total_utts:,} utterances \"\n",
        "                  f\"({elapsed:.1f} s elapsed)\")\n",
        "\n",
        "    # raw loop finished – print an early 100 % notice\n",
        "    core_done_time = time.time() - start_time\n",
        "    print(\n",
        "        f\"[100 %] raw segmentation complete in {core_done_time:.1f} s – \"\n",
        "        \"assembling results (converting chunks → phonemes, \"\n",
        "        \"building parent lexicon, computing new-words …)\"\n",
        "    )\n",
        "\n",
        "    # format final segmentation\n",
        "    final_segmentation = [\"\".join([chars_out[unit] for unit in u]) for u in final_segmentation]\n",
        "    final_segmentation = [\" \".join(f\"({seg})\" for seg in s.split(\"|\")) for s in final_segmentation]\n",
        "\n",
        "    # convert each lexicon entry from character format back to the original phonemic/syllabic units\n",
        "    lexicon_by_utterance_out = []\n",
        "\n",
        "    for lexicon in lexicon_by_utterance:\n",
        "        converted_lexicon = []\n",
        "        for chunk in lexicon:\n",
        "            # convert each character in the chunk to the original unit\n",
        "            units = [chars_out[c] for c in chunk]\n",
        "            converted_lexicon.append(\" \".join(units))\n",
        "        lexicon_by_utterance_out.append(converted_lexicon)\n",
        "\n",
        "    # Create the DataFrame with proper decoded chunks\n",
        "    df = pd.DataFrame({\n",
        "        \"cds_utterance_id\": range(1, len(utterances_orig) + 1),\n",
        "        \"utterance_phonemes\": utterances_orig,\n",
        "        \"encoded_input\": final_segmentation,\n",
        "        \"new_chunks\": lexicon_by_utterance_out\n",
        "    })\n",
        "\n",
        "    # Build cumulative parent lexicon for each utterance\n",
        "    parent_lexicon_progressive = []\n",
        "    words_seen_so_far = set()\n",
        "\n",
        "    pattern = re.compile(r\"\\((.*?)\\)\")\n",
        "    parent_lexicon_progressive = []\n",
        "    words_seen_so_far = set()\n",
        "\n",
        "    for utt in utterances_orig:\n",
        "        words_seen_so_far.update(pattern.findall(utt))\n",
        "        parent_lexicon_progressive.append(words_seen_so_far.copy())\n",
        "\n",
        "    # Add a column with the learned words (i.e. parent words that appear as substrings of any learned chunk)\n",
        "    # Use the cumulative parent lexicon\n",
        "    # Precompute for speed\n",
        "    chunks_col = df[\"new_chunks\"].tolist()\n",
        "    words_col = parent_lexicon_progressive\n",
        "\n",
        "    # Fast match using set logic and substring lookup\n",
        "    def find_learned_words(chunks, allowed_words):\n",
        "        learned = set()\n",
        "        for word in allowed_words:\n",
        "            for chunk in chunks:\n",
        "                if word in chunk:\n",
        "                    learned.add(word)\n",
        "                    break  # no need to check other chunks\n",
        "        return sorted(learned)\n",
        "\n",
        "    learned_words_so_far = set()\n",
        "    new_learned_words_all = []\n",
        "\n",
        "    for chunks, allowed_words in zip(chunks_col, words_col):\n",
        "        new_learned = []\n",
        "        for word in allowed_words:\n",
        "            if word in learned_words_so_far:\n",
        "                continue\n",
        "            if any(word in chunk for chunk in chunks):\n",
        "                new_learned.append(word)\n",
        "                learned_words_so_far.add(word)\n",
        "        new_learned_words_all.append(sorted(new_learned))\n",
        "\n",
        "    df[\"new_words\"] = new_learned_words_all\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"[100 %] finished processing {total_utts:,} utterances \"\n",
        "          f\"in {total_time:.1f} s ({total_time/60:.1f} min)\")\n",
        "\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "18iNJv6JLpOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, play with Mini-CLASSIC\n",
        "# Inspect the chunks learned, the words learned, and modify its parameters.\n",
        "\n",
        "# Notice how the output of the model is in a table format. How neat is that!\n",
        "# The table is specifically a Pandas DataFrame, which is a powerful and widely-used data structure in Python for\n",
        "# organising and analysing tabular data. It allows you to easily view, filter, group, and summarise the model's output.\n",
        "\n",
        "# Pro Tip:\n",
        "# to fully visualise the table, once you've run the cell, click on the blue table icon that appears to the left of the column names in the output.\n",
        "# This will open an interactive preview of the DataFrame.\n",
        "\n",
        "utterances_example = [ # For the sake of the example, let's use orthographic utterances, as they are easier to read than phonemic transcriptions.\n",
        "    \"(w h e r e s) (d a d d y)\",\n",
        "    \"(w h e r e s) (d a d d y)\",\n",
        "    \"(w h e r e s) (d a d d y)\"\n",
        "]\n",
        "\n",
        "model_df_example = run_chunking_model(utterances_example, learning_rate=1, final_bias_strength=0, set_seed=1234)\n",
        "model_df_example"
      ],
      "metadata": {
        "id": "6Nse2wDKaNvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Section 2: Mother and Child Data\n",
        "\n",
        "**Objective**\n",
        "Get hands-on with one mother–child dyad from the Manchester Corpus.\n",
        "\n",
        "**Required Python Knowledge**\n",
        "\n",
        "* Fetching remote files (`requests`, `pd.read_excel`)\n",
        "* Regex\n",
        "* Core Pandas operations: `apply`, `explode`, `groupby`, `qcut`, basic DataFrame maths\n",
        "* List/set comprehensions\n",
        "* Simple plotting with Matplotlib (`plot`, axes labels, ticks, legend, `tight_layout`)\n",
        "\n"
      ],
      "metadata": {
        "id": "aytr7DEw8cMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brilliant # 👶 Let's take a first look at the input to our Mini-CLASSIC model!\n",
        "\n",
        "# In their 2017 study, Jones and Rowland used real conversations between mothers and children from\n",
        "# the Manchester Corpus. To keep things simple (and fast), we’ll work with just one of these conversations (target child = Becky).\n",
        "\n",
        "# 🧾 Your task:\n",
        "# We'll begin by downloading a small extract from the Manchester Corpus. This will help you understand\n",
        "# the structure of the data we’ll be working with later.\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/francescocabiddu/MEDALWorkshopChildComputational/refs/heads/main/Eng-UK_conversation-flow_219_Manchester_15410_Becky_Example.txt\"\n",
        "\n",
        "# 🔗 Dataset URL:\n",
        "# The file contains lines with three elements:\n",
        "# - a speaker role (either Child_Directed_Speech or Target_Child),\n",
        "# - an orthographic transcription like (give) (dolly) (some),\n",
        "# - and a phonemic transcription like (ɡ ɪ v) (d ɒ l i) (s ɐ m)\n",
        "\n",
        "# 👇 Pro tips:\n",
        "# - Use the `requests` library to download the text file from the URL.\n",
        "# - Load the text into a variable.\n",
        "# - Split the content into individual lines — each line is one utterance.\n",
        "# - Then, print the lines.\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "vT8nDUszsjU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Send a GET request to the URL to retrieve the content of the text file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Extract the content of the response as plain text\n",
        "utterances_text = response.text\n",
        "\n",
        "# Split lines into a list\n",
        "utterances_lines = utterances_text.strip().split(\"\\n\")\n",
        "\n",
        "# Print utterances\n",
        "utterances_lines\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "cnOO4Zq9tSEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧒 So cool! Now that you’ve explored how conversation data is structured,\n",
        "# let’s work with two separate files: one for the child’s speech and one for the mother's speech.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# We’ll now load two Excel files:\n",
        "# 1️⃣ One contains phonemic utterances produced by the child (Becky).\n",
        "# 2️⃣ The other contains phonemic utterances from the mother (Child-Directed Speech).\n",
        "\n",
        "# 🧠 Why only phonemes?\n",
        "# For our simulations, we only need the phonemic representations of the utterances — so we’ll keep things simple and focus on those.\n",
        "\n",
        "# 📁 File format:\n",
        "# Unlike the previous `.txt` file, these are `.xlsx` Excel files. This is a good chance to practise importing from a different format!\n",
        "\n",
        "# 👇 Pro tips:\n",
        "# - Use the `pd.read_excel()` function to load the Excel files directly from the provided URLs.\n",
        "# - Store the child data in a DataFrame called `child_df`\n",
        "# - Store the child-directed speech in a DataFrame called `cds_df`\n",
        "# - Preview the first few rows of each dataset using `.head()`\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n",
        "\n",
        "becky_url = \"https://github.com/francescocabiddu/MEDALWorkshopChildComputational/raw/refs/heads/main/becky_speech.xlsx\"\n",
        "becky_cds_url = \"https://github.com/francescocabiddu/MEDALWorkshopChildComputational/raw/refs/heads/main/becky_cds.xlsx\"\n",
        "\n"
      ],
      "metadata": {
        "id": "szp_pGpSwb_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Read Excel files directly from URL\n",
        "child_df = pd.read_excel(becky_url)\n",
        "cds_df = pd.read_excel(becky_cds_url)\n",
        "\n",
        "# Preview the data\n",
        "child_df.head()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "u6xAzwNR0Vm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Now let’s compute some basic summary statistics to better understand our conversation samples.\n",
        "\n",
        "# We’ll compare the speech of the child and the mother across a few key measures.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# Create a small table that includes the following statistics for both the child and the mother:\n",
        "\n",
        "# 1️⃣ Total number of utterances\n",
        "# 2️⃣ Mean length of utterance (in number of words)\n",
        "# 3️⃣ Total number of word tokens (all words produced, including repetitions)\n",
        "# 4️⃣ Total number of word types (unique words only)\n",
        "\n",
        "# 👇 Pro tips:\n",
        "# - Each row in `child_df` and `cds_df` contains a phonemic utterance in the format: (w ʌ n ə) (t ɛ d i)\n",
        "# - Use regex to extract the bracketed word units.\n",
        "# - Count the number of utterances using `len()`\n",
        "# - Count word tokens by flattening the extracted words and taking the total\n",
        "# - Count word types using a `set()`\n",
        "\n",
        "# 🛠️ Output:\n",
        "# Create a small summary DataFrame or dictionary to neatly display the stats for child and mother.\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "BIvvFdQq0mQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract words from a phonemic utterance like: (w ʌ n ə) (t ɛ d i)\n",
        "def extract_words(utterance_str):\n",
        "    return re.findall(r\"\\((.*?)\\)\", utterance_str)\n",
        "\n",
        "# Helper function to compute summary stats for a dataframe\n",
        "def compute_summary(df):\n",
        "    # Check for missing utterances and raise an error if any are found\n",
        "    if df['utterance_phonemes'].isna().any():\n",
        "        raise ValueError(\"Missing utterances found in 'utterance_phonemes' column!\")\n",
        "\n",
        "    # Apply extraction directly to the column\n",
        "    word_lists = df['utterance_phonemes'].apply(extract_words)\n",
        "\n",
        "    total_utterances = len(word_lists)\n",
        "    mean_length = word_lists.apply(len).mean()\n",
        "    all_tokens = sum(word_lists, [])  # flatten list\n",
        "    num_tokens = len(all_tokens)\n",
        "    num_types = len(set(all_tokens))\n",
        "\n",
        "    return {\n",
        "        \"total_utterances\": total_utterances,\n",
        "        \"mean_utterance_length\": round(mean_length, 2),\n",
        "        \"word_tokens\": num_tokens,\n",
        "        \"word_types\": num_types\n",
        "    }\n",
        "\n",
        "# Compute stats\n",
        "child_stats = compute_summary(child_df)\n",
        "mother_stats = compute_summary(cds_df)\n",
        "\n",
        "# Combine into DataFrame for display\n",
        "summary_df = pd.DataFrame([child_stats, mother_stats], index=[\"Child\", \"Mother\"])\n",
        "summary_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "_lTDGPbT3a40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Excellent! As we saw, the mother speaks more often than the child, uses longer utterances,\n",
        "# and draws from a wider vocabulary — exactly what one might expect in child-directed speech.\n",
        "\n",
        "# 📈 In Jones & Rowland’s simulations, the key focus was on vocabulary growth — specifically,\n",
        "# how *new word types* are learned over time.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# Let’s now write a function that tracks *new word types* as they appear in the dataset.\n",
        "# For each utterance, the function should identify which words are being used for the *first time* in the conversation.\n",
        "\n",
        "# 🛠️ What counts as a \"word\"?\n",
        "# For our purposes, each bracketed unit in the phonemic transcription (e.g., (b æ n ə n ə))\n",
        "# is treated as one \"word\".\n",
        "\n",
        "# 👇 Step-by-step instructions:\n",
        "# - Define a function `add_new_words_column()` that:\n",
        "#     • Takes a DataFrame with a column of utterance phonemes\n",
        "#     • Extracts the bracketed word units\n",
        "#     • Tracks which words have already been seen\n",
        "#     • Adds a new column showing which words are *new* in each row\n",
        "# - Apply the function to both `child_df` and `cds_df`\n",
        "# - Preview the result to check that it’s working\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "9YF13eNh3plh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def add_new_words_column(df, phoneme_col='utterance_phonemes', new_col='new_words'):\n",
        "    import re\n",
        "    if df[phoneme_col].isna().any():\n",
        "        raise ValueError(f\"Missing values in '{phoneme_col}'\")\n",
        "\n",
        "    def extract_words(s):\n",
        "        return re.findall(r\"\\((.*?)\\)\", s)\n",
        "\n",
        "    seen_global = set()\n",
        "    new_lists   = []\n",
        "\n",
        "    for utter in df[phoneme_col]:\n",
        "        current     = extract_words(utter)\n",
        "\n",
        "        current_unique = list(dict.fromkeys(current))\n",
        "\n",
        "        new_in_row = [w for w in current_unique if w not in seen_global]\n",
        "        seen_global.update(current_unique)\n",
        "        new_lists.append(new_in_row)\n",
        "\n",
        "    df[new_col] = new_lists\n",
        "    return df\n",
        "\n",
        "# Apply to both child and mother speech\n",
        "child_df = add_new_words_column(child_df)\n",
        "cds_df = add_new_words_column(cds_df)\n",
        "\n",
        "# Preview the result\n",
        "child_df[['utterance_phonemes', 'new_words']].iloc[0:10]\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "IG9EKjeJJwwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Great, let's double check that the new column only contains unique words not produced in previous utterances\n",
        "def check_global_uniqueness(df, col_name):\n",
        "    \"\"\"\n",
        "    Checks if all entries in a column of lists are globally unique (i.e., no duplicates across rows).\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - col_name: name of the column containing lists of items (e.g., new_words or new_chunks)\n",
        "\n",
        "    Returns:\n",
        "    - None, but prints a summary and whether duplicates were found\n",
        "    \"\"\"\n",
        "    # Flatten all lists into a single list\n",
        "    all_items = sum(df[col_name], [])\n",
        "\n",
        "    # Compute total and unique counts\n",
        "    total_count = len(all_items)\n",
        "    unique_count = len(set(all_items))\n",
        "\n",
        "    # Print result\n",
        "    if total_count == unique_count:\n",
        "        print(f\"✅ No duplicates in column '{col_name}' — all entries are globally unique.\")\n",
        "    else:\n",
        "        print(f\"❌ Duplicates found in column '{col_name}'.\")\n",
        "        print(f\"Total entries: {total_count}\")\n",
        "        print(f\"Unique entries: {unique_count}\")\n",
        "        print(f\"Duplicates: {total_count - unique_count}\")\n",
        "\n",
        "\n",
        "print(\"Check on child_df...\")\n",
        "check_global_uniqueness(child_df, \"new_words\")\n",
        "print(\"Check on cds_df...\")\n",
        "check_global_uniqueness(cds_df, \"new_words\")"
      ],
      "metadata": {
        "id": "z3gJY0XHL97l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🗂️ Almost there! To mirror Jones & Rowland’s vocabulary-growth plot, we now need a sense of *time*.\n",
        "\n",
        "# 📈 In their simulations, the “quantity” effect emerged very early.\n",
        "# To capture such early differences, we’ll divide each conversation into a *large* number of\n",
        "# equally sized stages — say, 100.  That way, we can inspect vocabulary growth from the\n",
        "# very first few utterances right through to the end.\n",
        "\n",
        "# 🧾 Pro tips:\n",
        "# 1️⃣ Add an \"utterance_id\" column to each DataFrame that simply numbers the utterances in order (starting at 1).\n",
        "# 2️⃣ Use `pd.qcut()` to split those IDs into 100 equal-sized *stages*.\n",
        "#    • Label the stages 1-to-100 (tip: `labels=False` gives 0-based labels; add 1 afterward).\n",
        "# 3️⃣ Store the stage labels in a new column called \"stage\".\n",
        "# 4️⃣ Preview the updated child DataFrame to check your work.\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n",
        "\n"
      ],
      "metadata": {
        "id": "l8vIycxhcvET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "child_df[\"utterance_id\"] = range(1, len(child_df) + 1)\n",
        "child_df[\"stage\"] = pd.qcut(child_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "\n",
        "cds_df[\"utterance_id\"] = range(1, len(cds_df) + 1)\n",
        "cds_df[\"stage\"] = pd.qcut(cds_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "\n",
        "child_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "ZXpps0k0eokx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Time to watch production vocabulary grow!\n",
        "\n",
        "# We’ve marked which word types are *new* in each utterance and binned the conversation\n",
        "# into 100 stages.  Now we want to count how the *cumulative* number of unique word\n",
        "# types increases across those stages, just as Jones & Rowland did.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Write a function `compute_cumulative_new_words(df, stage_col='stage', words_col='new_words')`\n",
        "#    that returns a list of length 100, where each element is the total number of\n",
        "#    *unique* word types encountered up to (and including) that stage.\n",
        "#    • Hint: `df.explode(words_col)` will expand the list column into one word per row.\n",
        "#    • Drop rows where the exploded value is NaN (utterances with zero new words).\n",
        "#    • Iterate through stages 1…100, keep a running `set()` of words you have “seen”.\n",
        "# 2️⃣ Use the function to compute two lists:\n",
        "#       • `stage_counts_cds`   – mother’s cumulative words produced\n",
        "#       • `stage_counts_child` – child’s cumulative words produced\n",
        "\n",
        "# 👇 Skeleton to get you started:\n",
        "#\n",
        "# def compute_cumulative_new_words(df, stage_col='stage', words_col='new_learned_words'):\n",
        "#     # YOUR CODE HERE\n",
        "#     return cumulative_counts\n",
        "#\n",
        "# stage_counts_cds   = compute_cumulative_new_words(cds_df)\n",
        "# stage_counts_child = compute_cumulative_new_words(child_df)\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "SF4edUp4cMR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "def compute_cumulative_new_words(df, stage_col='stage', words_col='new_words'):\n",
        "    # Explode and drop rows with no new words\n",
        "    df_exploded = df.explode(words_col).dropna(subset=[words_col])\n",
        "    \n",
        "    cumulative_counts = []\n",
        "    seen_words = set()\n",
        "    \n",
        "    for stage in sorted(df[stage_col].unique()):\n",
        "        current_words = set(df_exploded[df_exploded[stage_col] == stage][words_col])\n",
        "        seen_words.update(current_words)\n",
        "        cumulative_counts.append(len(seen_words))\n",
        "    \n",
        "    return cumulative_counts\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute cumulative counts\n",
        "stage_counts_cds = compute_cumulative_new_words(cds_df)\n",
        "stage_counts_child = compute_cumulative_new_words(child_df)\n",
        "\n",
        "# Preview child counts for the first 20 stages\n",
        "stage_counts_child[0:20]\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "xrLLMldFhRWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me do some basic sanity checks on the cumulative counts\n",
        "def _quick_check(df, counts):\n",
        "    assert len(counts) == df['stage'].nunique() == 100, \"Should have 100 stage counts\"\n",
        "    assert all(x <= y for x, y in zip(counts, counts[1:])), \"Counts must be non-decreasing\"\n",
        "    final_unique = len(set().union(*df['new_words']))\n",
        "    assert counts[-1] == final_unique, \"Final count should equal total unique word types\"\n",
        "\n",
        "_quick_check(cds_df, stage_counts_cds)\n",
        "_quick_check(child_df, stage_counts_child)\n",
        "print(\"✅  Cumulative counts look consistent!\")"
      ],
      "metadata": {
        "id": "V48pw7d6gqDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📈 With `stage_counts_cds` (mother) and `stage_counts_child` (child) ready,\n",
        "# it’s time to plot the cumulative vocabulary growth for both speakers.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Use Matplotlib to plot two lines:\n",
        "#     • Mother (CDS)\n",
        "#     • Child\n",
        "# 2️⃣ The x-axis should be the stage number (1 → 100).\n",
        "# 3️⃣ Label the axes clearly:\n",
        "#     • x-label: 'Stage'\n",
        "#     • y-label: e.g., 'Cumulative Count of Unique Words Produced'\n",
        "# 4️⃣ Add a legend identifying the two curves.\n",
        "# 5️⃣ For readability, set the x-tick labels every 10 stages (0, 10, 20, …, 100).\n",
        "# 6️⃣ Include a grid and call `plt.tight_layout()` before displaying with `plt.show()`.\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "T3oyKieMhtsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(1, len(stage_counts_cds) + 1), stage_counts_cds, marker='o', label='CDS')\n",
        "plt.plot(range(1, len(stage_counts_child) + 1), stage_counts_child, marker='s', label='Child')\n",
        "\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Cumulative Count of Unique Words Produced')\n",
        "plt.xticks(range(0, len(stage_counts_cds) + 1, 10))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "2mvqqmG8jNCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Section 3: Run Mini-CLASSIC\n",
        "\n",
        "**Objective**\n",
        "Train the Mini-CLASSIC learner on Becky’s maternal input, inspect what it learns, and tune its hyper-parameters.\n",
        "\n",
        "**Required Python Knowledge**\n",
        "\n",
        "* Calling functions with keyword arguments and reproducible seeds\n",
        "* Pandas column wrangling (`assign`, `range`, `qcut`)\n",
        "* Basic Matplotlib line plotting (multiple series, legends, axis labels, grids)\n",
        "* (optional) RMSE calculation with NumPy\n"
      ],
      "metadata": {
        "id": "xjq9lW0X84LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🤖 Time to let Mini-CLASSIC learn!\n",
        "\n",
        "# So far, we’ve plotted *produced* word types for mother and child.\n",
        "# Next, we’ll see how our computational model learns from the mother’s input.\n",
        "\n",
        "# ⏳ Heads-up: Depending on connection speed, running the model may take a couple of minutes.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Call `run_chunking_model()` on the mother’s phonemic utterances:\n",
        "#        • input          → `cds_df[\"utterance_phonemes\"]`\n",
        "#        • learning_rate  → 1\n",
        "#        • final_bias_strength → 0\n",
        "#        • set_seed       → 1234  (makes results reproducible)\n",
        "# 2️⃣ Store the result in a DataFrame named `model_df`.\n",
        "# 3️⃣ Display `model_df` to inspect what the function returns\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "iSpPpKzCzY98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "model_df = run_chunking_model(cds_df[\"utterance_phonemes\"], learning_rate=1, final_bias_strength=0, set_seed=1234)\n",
        "model_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "qVt5UpmSmcpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Great, now let's add the stage column to model_df\n",
        "# and plot the model acquired vocabulary alongside mother and child\n",
        "\n",
        "# Add Stage\n",
        "model_df[\"utterance_id\"] = range(1, len(model_df) + 1)\n",
        "model_df[\"stage\"] = pd.qcut(model_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "\n",
        "# Compute cumulative counts for the model\n",
        "stage_counts_model = compute_cumulative_new_words(model_df)\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(1, len(stage_counts_cds) + 1), stage_counts_cds, marker='o', label='CDS')\n",
        "plt.plot(range(1, len(stage_counts_child) + 1), stage_counts_child, marker='s', label='Child')\n",
        "plt.plot(range(1, len(stage_counts_model) + 1), stage_counts_model, marker='s', label='Model')\n",
        "\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Cumulative Count of Unique Words Produced')\n",
        "plt.xticks(range(0, len(stage_counts_cds) + 1, 10))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7UWmAfFkGOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠️ Model tuning time!\n",
        "\n",
        "# With `learning_rate = 1` and `final_bias_strength = 0`, the model is acquiring more\n",
        "# word types than the child.  Before we explore quantity and diversity manipulations, I think it would\n",
        "# be a good idea to have a *baseline* model that captures the child’s vocabulary growth as closely as possible.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Go back two cells, and experiment with several values for `learning_rate` (e.g., 0.2, 0.5, 0.6, 0.7, 0.8)\n",
        "#    and `final_bias_strength` (e.g., 0.5, 1.0, 1.5, 2).\n",
        "# 2️⃣ For each parameter pair, replot the model curve together with the mother and child curve\n",
        "# 3️⃣ Visually identify the parameter combination that brings the model curve closest to the child curve.\n",
        "# 4️⃣ When you’re satisfied, keep the *best-fitting* parameter set for later analyses\n",
        "#     and store the corresponding model output in `model_df`.\n",
        "\n",
        "# 👉 Hints:\n",
        "# - Use a small grid search (a handful of combos is fine for now).\n",
        "# - You might judge “fit” by eye, or compute the Root Mean Squared Error between curves if you really fancy.\n",
        "\n",
        "# ✏️ CONTINUE WITH NEXT CELL WHEN SATIFIED WITH MODEL FIT TO CHILD DATA"
      ],
      "metadata": {
        "id": "sjHfiLxNnhGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# This is a reasonably good fit to child data\n",
        "model_df = run_chunking_model(cds_df[\"utterance_phonemes\"], learning_rate=0.6, final_bias_strength=1, set_seed=1234)\n",
        "model_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "9nAyUtxOrMcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Section 4: Quantity and Diversity Input\n",
        "\n",
        "**Objective**\n",
        "Import two experimentally manipulated versions of the mother’s speech—a *quantity* boost and a *diversity* boost—and create a concise quantitative profile for each.\n",
        "\n",
        "**Required Python knowledge**\n",
        "\n",
        "* Reading remote Excel files with **`pd.read_excel()`** and adding basic identifier columns (`range`, `.assign`)\n",
        "* Binning rows with **`pd.qcut()`** and using **`groupby()`** to aggregate stage-wise counts\n",
        "* Extracting words via regular expressions, plus list handling and set logic\n",
        "* Writing small helper functions\n",
        "* Joining, renaming and generally tidying DataFrames\n"
      ],
      "metadata": {
        "id": "xZVdhqgh9QRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🌱 New input conditions: QUANTITY vs DIVERSITY\n",
        "\n",
        "# We’ve tuned a baseline Mini-CLASSIC model on the original maternal input.\n",
        "# Now we’ll import two *manipulated* versions of that input:\n",
        "#\n",
        "# 1️⃣ Quantity boost  – same vocabulary, but many utterances repeated → more *tokens*.\n",
        "# 2️⃣ Diversity boost – every other utterance swapped for one of the same length\n",
        "#                      from another mother but containing *new* words → more *types*.\n",
        "#\n",
        "# 🔗 Data sources:\n",
        "quantity_url  = \"https://github.com/francescocabiddu/MEDALWorkshopChildComputational/raw/refs/heads/main/becky_cds_quantity.xlsx\"\n",
        "diversity_url = \"https://github.com/francescocabiddu/MEDALWorkshopChildComputational/raw/refs/heads/main/becky_cds_diversity.xlsx\"\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Read each Excel file into its own DataFrame:\n",
        "#       • call them `cds_quantity_df` and `cds_diversity_df`\n",
        "# 2️⃣ Preview each DataFrame to make sure everything looks right.\n",
        "#\n",
        "# 📝 No “Pro Tips” this time — just reuse the code you wrote earlier!\n",
        "\n",
        "# ✏️ INSERT YOUR CODE BELOW\n",
        "\n"
      ],
      "metadata": {
        "id": "7GdPPC259fzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Read Excel files directly from URL\n",
        "cds_quantity_df = pd.read_excel(quantity_url)\n",
        "cds_diversity_df = pd.read_excel(diversity_url)\n",
        "\n",
        "# Display one of the DFs\n",
        "cds_quantity_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "97H4QpE-D4__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Summarise the three maternal input conditions\n",
        "\n",
        "# We now have three DataFrames:\n",
        "#   1️⃣ cds_df              – original maternal speech (baseline)\n",
        "#   2️⃣ cds_quantity_df     – quantity-boost version\n",
        "#   3️⃣ cds_diversity_df    – diversity-boost version\n",
        "#\n",
        "# 🧾 Your task:\n",
        "# Compute (for each DataFrame) the following four statistics:\n",
        "#   • total_utterances\n",
        "#   • mean_utterance_length          (average words per utterance)\n",
        "#   • word_tokens                    (all words, including repetitions)\n",
        "#   • word_types                     (unique words only)\n",
        "#\n",
        "# ➡ Feel free to reuse whatever approach you used earlier for the\n",
        "#    standard input — whether that was with helper functions or\n",
        "#    inline code.  The goal is simply to produce the same numbers\n",
        "#    for all three conditions.\n",
        "#\n",
        "# 1️⃣ Calculate the four stats for each DataFrame.\n",
        "# 2️⃣ Combine the results into a single summary table (e.g., a\n",
        "#    small pandas DataFrame) with rows labelled:\n",
        "#         \"Mother (standard)\", \"Mother – Quantity\", \"Mother – Diversity\"\n",
        "# 3️⃣ Display the table so you can compare how the manipulations\n",
        "#    have changed tokens and types.\n",
        "#\n",
        "# ✏️ INSERT YOUR CODE BELOW\n",
        "\n"
      ],
      "metadata": {
        "id": "-LQyWsm3CUva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract words from a phonemic utterance like: (w ʌ n ə) (t ɛ d i)\n",
        "def extract_words(utterance_str):\n",
        "    return re.findall(r\"\\((.*?)\\)\", utterance_str)\n",
        "\n",
        "# Helper function to compute summary stats for a dataframe\n",
        "def compute_summary(df):\n",
        "    # Check for missing utterances and raise an error if any are found\n",
        "    if df['utterance_phonemes'].isna().any():\n",
        "        raise ValueError(\"Missing utterances found in 'utterance_phonemes' column!\")\n",
        "\n",
        "    # Apply extraction directly to the column\n",
        "    word_lists = df['utterance_phonemes'].apply(extract_words)\n",
        "\n",
        "    total_utterances = len(word_lists)\n",
        "    mean_length = word_lists.apply(len).mean()\n",
        "    all_tokens = sum(word_lists, [])  # flatten list\n",
        "    num_tokens = len(all_tokens)\n",
        "    num_types = len(set(all_tokens))\n",
        "\n",
        "    return {\n",
        "        \"total_utterances\": total_utterances,\n",
        "        \"mean_utterance_length\": round(mean_length, 2),\n",
        "        \"word_tokens\": num_tokens,\n",
        "        \"word_types\": num_types\n",
        "    }\n",
        "\n",
        "# Compute stats\n",
        "cds_stats = compute_summary(cds_df)\n",
        "cds_quantity_stats = compute_summary(cds_quantity_df)\n",
        "cds_diversity_stats = compute_summary(cds_diversity_df)\n",
        "\n",
        "# Combine into DataFrame for display\n",
        "summary_df = pd.DataFrame([cds_stats,\n",
        "                           cds_quantity_stats,\n",
        "                           cds_diversity_stats],\n",
        "                          index=[\"Mother\", \"Mother - Quantity\",\n",
        "                                 \"Mother - Diversity\"])\n",
        "summary_df\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "LQIpDrZBFCWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Stage-by-stage vocabulary profile\n",
        "\n",
        "# So far, you’ve compared the three maternal input conditions (standard, quantity-boost,\n",
        "# diversity-boost) with *global* statistics.  Now we want to zoom in and see how\n",
        "# word tokens and word types accumulate at each of our 100 stages.\n",
        "#\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ **Add a “new_words” column** to both `cds_quantity_df` and `cds_diversity_df`\n",
        "#    (use the same method you used earlier for the baseline `cds_df`).\n",
        "#\n",
        "# 2️⃣ Write a helper function called `summarise_new_types_and_tokens` that,\n",
        "#    for a given DataFrame, returns a summary table with *one row per stage* and two columns:\n",
        "#        • word_types        – number of *new* word types produced in that stage\n",
        "#        • word_tokens       – total word tokens in that stage (incl. repetitions)\n",
        "#\n",
        "# 3️⃣ Apply the helper to all three DataFrames to obtain three per-stage tables.\n",
        "#\n",
        "# 4️⃣ Merge the results into a single table with columns:\n",
        "#        stage,\n",
        "#        cds_types,      cds_quantity_types,      cds_diversity_types,\n",
        "#        cds_tokens,     cds_quantity_tokens,     cds_diversity_tokens\n",
        "#\n",
        "# 5️⃣ Display the table for the **first 10 stages** only (use `.head(10)`).\n",
        "#\n",
        "# 🪄 Pro Tips:\n",
        "#   👉 Using .groupby():\n",
        "#   When you want to compute something separately for each stage (like word counts),\n",
        "#   you can use `.groupby('stage')` to group the DataFrame by the stage number.\n",
        "\n",
        "#   Example:\n",
        "#   for stage, group in df.groupby('stage'):\n",
        "#       # 'stage' is the current stage number\n",
        "#       # 'group' is a smaller DataFrame with only the rows for that stage\n",
        "\n",
        "# - To count word types: for each stage, combine all the lists in the 'new_words' column.\n",
        "# - To count tokens: extract all words from the 'utterance_phonemes' column and count how many there are.\n",
        "# - You can rename columns using `.rename(columns={...})`.\n",
        "# - To merge the tables: use `.join()` to combine them by their shared 'stage' index.\n",
        "# - To reorder columns: make a list with the order you want and use `df = df[that_list]`.\n",
        "#\n",
        "#\n",
        "# ✏️ INSERT YOUR CODE BELOW\n",
        "\n"
      ],
      "metadata": {
        "id": "j3xWLBHwFU4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def extract_words(utterance_str):\n",
        "    \"\"\"Return list of bracketed words from a phonemic utterance.\"\"\"\n",
        "    return re.findall(r\"\\((.*?)\\)\", utterance_str)\n",
        "\n",
        "def summarise_new_words_and_tokens(df, stage_col='stage',\n",
        "                                   new_words_col='new_words',\n",
        "                                   phoneme_col='utterance_phonemes'):\n",
        "    \"\"\"\n",
        "    Return a per-stage DataFrame with counts of unique new word types and total tokens.\n",
        "    \"\"\"\n",
        "    unique_types = {}\n",
        "    token_counts = {}\n",
        "    \n",
        "    for stage, grp in df.groupby(stage_col):\n",
        "        # types introduced *in this stage*\n",
        "        stage_new_types = set(sum(grp[new_words_col], []))\n",
        "        unique_types[stage] = len(stage_new_types)\n",
        "        \n",
        "        # all tokens (including repetitions)\n",
        "        token_list = sum(grp[phoneme_col].apply(extract_words), [])\n",
        "        token_counts[stage] = len(token_list)\n",
        "    \n",
        "    summary = pd.DataFrame({\n",
        "        'word_types': pd.Series(unique_types),\n",
        "        'word_tokens': pd.Series(token_counts)\n",
        "    }).sort_index()\n",
        "    summary.index.name = 'stage'\n",
        "    return summary\n",
        "\n",
        "# ---------- add new_words column ----------\n",
        "cds_quantity_df = add_new_words_column(cds_quantity_df)\n",
        "cds_diversity_df= add_new_words_column(cds_diversity_df)\n",
        "\n",
        "# ---------- add stage column ----------\n",
        "cds_quantity_df[\"utterance_id\"] = range(1, len(cds_quantity_df) + 1)\n",
        "cds_quantity_df[\"stage\"] = pd.qcut(cds_quantity_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "cds_diversity_df[\"utterance_id\"] = range(1, len(cds_diversity_df) + 1)\n",
        "cds_diversity_df[\"stage\"] = pd.qcut(cds_diversity_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "\n",
        "# ---------- per-stage summaries ----------\n",
        "std_stage  = summarise_new_words_and_tokens(cds_df)\n",
        "qty_stage  = summarise_new_words_and_tokens(cds_quantity_df)\n",
        "div_stage  = summarise_new_words_and_tokens(cds_diversity_df)\n",
        "\n",
        "# rename columns\n",
        "std_stage  = std_stage.rename(columns={'word_types':'cds_types',\n",
        "                                       'word_tokens':'cds_tokens'})\n",
        "qty_stage  = qty_stage.rename(columns={'word_types':'cds_quantity_types',\n",
        "                                       'word_tokens':'cds_quantity_tokens'})\n",
        "div_stage  = div_stage.rename(columns={'word_types':'cds_diversity_types',\n",
        "                                       'word_tokens':'cds_diversity_tokens'})\n",
        "\n",
        "# ---------- merge all three ----------\n",
        "summary_by_stage = (\n",
        "    std_stage\n",
        "    .join(qty_stage, how='outer')\n",
        "    .join(div_stage, how='outer')\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Re-order columns\n",
        "summary_by_stage = summary_by_stage[[\n",
        "    \"stage\",\n",
        "    \"cds_types\", \"cds_quantity_types\", \"cds_diversity_types\",\n",
        "    \"cds_tokens\", \"cds_quantity_tokens\", \"cds_diversity_tokens\"\n",
        "]]\n",
        "\n",
        "# Show the first 10 stages\n",
        "summary_by_stage.head(10)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "aPvYZSk5Mfab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Section 5: Testing the Effect of Quantity and Diversity\n",
        "\n",
        "**Objective**\n",
        "Run Mini-CLASSIC on the quantity-boost and diversity-boost corpora, plot their cumulative vocabulary curves alongside the baseline model.\n",
        "\n",
        "**Required Python knowledge**\n",
        "\n",
        "* Re-using previously defined functions and fixed model parameters\n",
        "* Calling custom functions on new datasets and storing their outputs\n",
        "* Basic list slicing (`counts[:20]`) and range manipulation\n",
        "* Multi-series plotting with Matplotlib: markers, line styles, legends and axis formatting\n",
        "* Light DataFrame housekeeping (adding stage labels, passing data into helper functions)\n"
      ],
      "metadata": {
        "id": "qHsz6Cu9iqB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Putting Mini-CLASSIC to the ultimate test!\n",
        "\n",
        "# You now know exactly how the three maternal inputs differ (standard, quantity-boost, diversity-boost).\n",
        "# Let’s see how those differences shape what the Mini-CLASSIC model learns.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Run `run_chunking_model()` on the **quantity** input (`cds_quantity_df[\"utterance_phonemes\"]`)\n",
        "#    and on the **diversity** input (`cds_diversity_df[\"utterance_phonemes\"]`) using **the same\n",
        "#    parameters** you identified for the best-fitting baseline model (learning_rate, final_bias_strength, seed).\n",
        "#\n",
        "# 2️⃣ For each model run:\n",
        "#    • Compute the cumulative vocabulary curve (unique word types learned)\n",
        "#      – use the same helper code you wrote for `model_df` earlier.\n",
        "#\n",
        "# 3️⃣ Plot three curves on the same graph:\n",
        "#    • Model trained on real CDS  (baseline model curve)\n",
        "#    • Model trained on quantity-boost input\n",
        "#    • Model trained on diversity-boost input\n",
        "#\n",
        "#    Label axes clearly (Stage vs Cumulative Unique Words), add a legend,\n",
        "#    and keep x-ticks every 10 stages.\n",
        "#\n",
        "#\n",
        "# 🪄 Pro Tips:\n",
        "# • You already have code to generate `stage_counts_*` from a model DataFrame – reuse it!\n",
        "# • To vary marker shapes: marker='o', marker='s', marker='^', marker='D'.\n",
        "# • Keep the colour palette simple – Matplotlib will choose distinct colours by default.\n",
        "# • If the curves overlap too much, try using dashed or dotted line styles\n",
        "#   (`linestyle='--'`, `'-.`', `':'`) in addition to different markers.\n",
        "#\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "cHYpXc4JBwxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "# Run models\n",
        "print(\"Running Model - Diversity...\")\n",
        "model_diversity_df = run_chunking_model(cds_diversity_df[\"utterance_phonemes\"], learning_rate=0.6, final_bias_strength=1, set_seed=1234)\n",
        "print(\"Running Model - Quantity...\")\n",
        "model_quantity_df = run_chunking_model(cds_quantity_df[\"utterance_phonemes\"], learning_rate=0.6, final_bias_strength=1, set_seed=1234)\n",
        "\n",
        "\n",
        "# Add Stage\n",
        "model_diversity_df[\"utterance_id\"] = range(1, len(model_diversity_df) + 1)\n",
        "model_diversity_df[\"stage\"] = pd.qcut(model_diversity_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "model_quantity_df[\"utterance_id\"] = range(1, len(model_quantity_df) + 1)\n",
        "model_quantity_df[\"stage\"] = pd.qcut(model_quantity_df[\"utterance_id\"], q=100, labels=False) + 1\n",
        "\n",
        "# Compute cumulative counts for the model\n",
        "stage_counts_model_diversity = compute_cumulative_new_words(model_diversity_df)\n",
        "stage_counts_model_quantity = compute_cumulative_new_words(model_quantity_df)\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(1, len(stage_counts_model) + 1), stage_counts_model, marker='s', label='Model')\n",
        "plt.plot(range(1, len(stage_counts_model_diversity) + 1), stage_counts_model_diversity, marker='s', label='Model - Diversity')\n",
        "plt.plot(range(1, len(stage_counts_model_quantity) + 1), stage_counts_model_quantity, marker='s', label='Model - Quantity')\n",
        "\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Cumulative Count of Unique Words Produced')\n",
        "plt.xticks(range(0, len(stage_counts_model) + 1, 10))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "Y-KWVpaV0JyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔍 Zooming in: early learning differences\n",
        "\n",
        "# The full cumulative plot showed that diversity drives stronger vocabulary growth overall,\n",
        "# while quantity provides a more modest boost – exactly in line with the findings of\n",
        "# Jones & Rowland (2017).  Well done on replicating that effect! 🎉\n",
        "#\n",
        "# There’s a subtler pattern, however:\n",
        "# 📌 In the *very first* stages, extra quantity may give the model a short-term edge\n",
        "#     over diversity – but it’s hard to see on the full-scale graph.\n",
        "#\n",
        "# To make this early effect clearer, we’ll zoom in on just the first 10 stages.\n",
        "\n",
        "# 🧾 Your task:\n",
        "# 1️⃣ Re-use the three cumulative curves you already computed:\n",
        "#       • `stage_counts_model`            – baseline (real CDS)\n",
        "#       • `stage_counts_model_quantity`   – quantity boost\n",
        "#       • `stage_counts_model_diversity`  – diversity boost\n",
        "#\n",
        "# 2️⃣ Slice each list to keep only the first 10 stages, e.g.  `counts[:10]`.\n",
        "#\n",
        "# 3️⃣ Plot the three curves again on a single graph:\n",
        "#\n",
        "# 🪄 Pro tips:\n",
        "# • Adjust the x-range to `range(1, 11)`.\n",
        "# • Copy/paste your earlier plotting code and tweak the list slices and labels.\n",
        "#\n",
        "# ✏️ INSERT YOUR CODE BELOW\n"
      ],
      "metadata": {
        "id": "6QxhzFHhotSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Reveal a solution</summary>\n",
        "  \n",
        "```python\n",
        "# Note: This is one possible solution, but other valid approaches may exist!\n",
        "\n",
        "# Compute cumulative counts for the model\n",
        "stage_counts_model_10 = stage_counts_model[:10]\n",
        "stage_counts_model_diversity_10 = stage_counts_model_diversity[:10]\n",
        "stage_counts_model_quantity_10 = stage_counts_model_quantity[:10]\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(1, len(stage_counts_model_10) + 1), stage_counts_model_10, marker='s', label='Model')\n",
        "plt.plot(range(1, len(stage_counts_model_diversity_10) + 1), stage_counts_model_diversity_10, marker='s', label='Model - Diversity')\n",
        "plt.plot(range(1, len(stage_counts_model_quantity_10) + 1), stage_counts_model_quantity_10, marker='s', label='Model - Quantity')\n",
        "\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Cumulative Count of Unique Words Produced')\n",
        "plt.xticks(range(0, len(stage_counts_model_10) + 1, 1))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "VcSaJ5tN0TQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 🚀 What We’ve Accomplished\n",
        "\n",
        "You worked with Becky’s mother–child corpus, tagging every word at its first appearance, slicing the data into equal “learning” stages and plotting cumulative vocabulary growth for mother, child and three Mini-CLASSIC models. After tuning a baseline model on real child-directed speech, you trained it on two altered inputs—one that simply repeated existing utterances (quantity) and one that injected new ones that expand the vocabulary used (diversity). The curves reproduced Jones and Rowland’s key result: extra quantity gives a brief initial boost, but greater lexical diversity soon overtakes it and drives superior long-term vocabulary growth.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 Why These Findings Matter\n",
        "\n",
        "1. **Quantity jump-starts learning, but diversity wins the long game**\n",
        "   *Jones and Rowland (2017) reported precisely this with the full CLASSIC model; your Mini-CLASSIC run hits the same crossover.*\n",
        "\n",
        "   * **Earliest stages (tiny lexicon):** more *tokens* mean more opportunities to reinforce the basic bi- and triphones from which words are built. Children (and chunking models) thrive on repetition when everything is still novel.\n",
        "   * **Soon afterwards:** once a decent sub-lexical scaffold exists, hearing *different* words is far more informative than hearing the same few again. Diversity fuels the combinatorial explosion that lets a modest chunk inventory cover thousands of forms.\n",
        "\n",
        "2. **A mechanistic link to processing-speed cascades in children**\n",
        "   Weisleder & Fernald ([2013](https://doi.org/10.1177/0956797613488145)) showed that richer caregiver talk predicts faster word recognition later on.\n",
        "\n",
        "   * **Why:** Diverse input → broader chunk inventory → novel words can be encoded in *fewer* chunks → faster processing → more cognitive resources to map sound to meaning.\n",
        "   * Quantity alone deepens representations of the *same* lexemes; it cannot create the adaptable chunks that accelerate later learning.\n",
        "\n",
        "3. **Methodological pay-off**\n",
        "   Manipulating real corpora while holding other statistics constant is notoriously difficult; simulation lets us pull single causal levers cleanly. Mini-CLASSIC provides a sandbox for bold “what-if” tests, delivering proof-of-principle evidence that can guide empirical work in language acquisition!\n",
        "\n",
        "---\n",
        "\n",
        "### 🔮 Points for Reflection\n",
        "\n",
        "* How would you **go about** repeating the analysis across *all* parent–child dyads in the Manchester corpus?\n",
        "* We fixed one learning-rate / bias combination. Are the results **independent** of that choice? How could you extend the analysis to explore a parameter grid (see Jones & Rowland for their approach)?\n",
        "* In this course, we’ve looked at the words learned by each model, but we haven’t yet examined the actual chunks that were learned. Exploring these chunks could help us better understand the quantity and diversity effects. For instance, an early advantage from quantity should reflect the fact that the model is exposed to more repetitive utterances, providing more opportunities to build word representations from sublexical chunks more quickly. If that’s the case, then we should also observe an early advantage in the learning of sublexical chunks. Think about how you could extract sublexical chunks from the Mini-CLASSIC model’s output (a sublexical chunk is any chunk that forms part of a word).\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 Bonus Track\n",
        "\n",
        "*Well done for completing this MEDAL workshop! Great work – now enjoy your musical reward!* 🎶\n",
        "\n",
        "<audio controls>\n",
        "  <source src=\"https://drive.google.com/uc?export=download&id=1uAyaNS1NfIqAC5Wc9xtesmUT3UctrpQU\" type=\"audio/mpeg\">\n",
        "  Your browser does not support the audio element.\n",
        "</audio>\n",
        "\n",
        " *by ChatGPT + SunoAI*\n",
        "\n"
      ],
      "metadata": {
        "id": "cyn9NXqxBDOM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kghfFWs76jXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}